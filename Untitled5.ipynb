{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba0a73a-b93e-4405-961b-1a8350c441a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to chatgpt1.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "def extract_profile_summary(text):\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\", \"Profile\"\n",
    "    ]\n",
    "    \n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.isupper():\n",
    "                break\n",
    "            extracted_lines.append(line)\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    # Regular expression pattern for phone numbers, allowing symbols like (), -, and spaces\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. ()]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    \n",
    "    # If a match is found, remove all non-digit characters to get a clean number\n",
    "    if match:\n",
    "        # Combine all groups into a single string and remove any non-numeric characters\n",
    "        raw_number = match.group(0)\n",
    "        clean_number = re.sub(r'\\D', '', raw_number)  # Remove all non-digit characters\n",
    "        return clean_number\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract address\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            return line.strip()\n",
    "    return \"Not Found\"\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "# Function to extract links and their associated text from a PDF\n",
    "def extract_links_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks along with the associated text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing separate lists for LinkedIn, GitHub, email, Kaggle, portfolio, Facebook, Instagram, Git, Monster, and other links.\n",
    "    \"\"\"\n",
    "    # Create a list to store links and associated text\n",
    "    links_with_text = []\n",
    "\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Iterate through each page in the PDF\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "\n",
    "            # Check if the page has annotations (i.e., links)\n",
    "            if '/Annots' in page:\n",
    "                annotations = page['/Annots']\n",
    "\n",
    "                # Extract links and associated text from each annotation\n",
    "                for annotation in annotations:\n",
    "                    annotation_object = annotation.get_object()\n",
    "                    if '/A' in annotation_object and '/URI' in annotation_object['/A']:\n",
    "                        link = annotation_object['/A']['/URI']\n",
    "                        # Try to get the text associated with the link\n",
    "                        text = annotation_object.get('/T', '') or annotation_object.get('/Contents', 'Unknown Text')\n",
    "                        links_with_text.append(f\"{text} - {link}\")\n",
    "\n",
    "    # Extract different types of links using patterns\n",
    "    linkedin_links = [link for link in links_with_text if 'linkedin.com' in link]\n",
    "    github_links = [link for link in links_with_text if 'github.com' in link]\n",
    "    kaggle_links = [link for link in links_with_text if 'kaggle.com' in link]\n",
    "    portfolio_links = [link for link in links_with_text if any(domain in link for domain in ['.com', '.net', '.org', '.io'])]\n",
    "    facebook_links = [link for link in links_with_text if 'facebook.com' in link]\n",
    "    instagram_links = [link for link in links_with_text if 'instagram.com' in link]\n",
    "    git_links = [link for link in links_with_text if 'git' in link]\n",
    "    monster_links = [link for link in links_with_text if 'monster.com' in link]\n",
    "\n",
    "    # Filter out email links (emails won't typically have clickable links in PDFs)\n",
    "    email_links = [link for link in links_with_text if '@' in link]\n",
    "\n",
    "    # Create a dictionary to store links by type\n",
    "    links_dict = {\n",
    "        'LinkedIn': linkedin_links,\n",
    "        'GitHub': github_links,\n",
    "        'Email': email_links,\n",
    "        'Kaggle': kaggle_links,\n",
    "        'Portfolio': portfolio_links,\n",
    "        'Facebook': facebook_links,\n",
    "        'Instagram': instagram_links,\n",
    "        'Git': git_links,\n",
    "        'Monster': monster_links,\n",
    "        'Other': [link for link in links_with_text if link not in linkedin_links and link not in github_links and link not in email_links and link not in kaggle_links and link not in portfolio_links and link not in facebook_links and link not in instagram_links and link not in git_links and link not in monster_links]\n",
    "    }\n",
    "\n",
    "    return links_dict\n",
    "\n",
    "\n",
    "# Updated Function to extract experience details\n",
    "def extract_experience(text):\n",
    "    experience_keywords = [\n",
    "        \"work history\", \"employment history\", \"work experience\",\n",
    "        \"professional experience\", \"career summary\", \"professional background\",\n",
    "        \"job experience\", \"internships\", \"relevant experience\",\n",
    "        \"contract work\", \"military experience\", \"volunteer work\"\n",
    "    ]\n",
    "    \n",
    "    date_patterns = [\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\",\n",
    "        r\"\\b\\d{4}\\s*[-–—to]+\\s*\\d{4}\\b\",\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*Present\",\n",
    "        r\"\\b\\d{4}\\s*[-–—to]+\\s*Present\\b\"\n",
    "    ]\n",
    "\n",
    "    job_title_keywords = [\n",
    "        \"manager\", \"director\", \"engineer\", \"analyst\", \"consultant\",\n",
    "        \"assistant\", \"coordinator\", \"specialist\", \"developer\", \"designer\",\n",
    "        \"executive\", \"advisor\", \"technician\", \"officer\", \"intern\", \"trainee\"\n",
    "    ]\n",
    "\n",
    "    company_patterns = r\"(at|with|for)\\s+([A-Z][\\w&,. ]+)\"\n",
    "    location_patterns = [\n",
    "        r\"\\bin\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b\",\n",
    "        r\"\\b([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\s+(USA|Inc|Ltd|LLC|Co)\\b\"\n",
    "    ]\n",
    "\n",
    "    location_regex = '|'.join(location_patterns)\n",
    "    experience_start = re.search('|'.join(experience_keywords), text, re.IGNORECASE)\n",
    "    if not experience_start:\n",
    "        return []\n",
    "\n",
    "    content = text[experience_start.start():]\n",
    "    dates = re.findall('|'.join(date_patterns), content)\n",
    "    job_entries = re.split('|'.join(date_patterns), content)\n",
    "    experiences = []\n",
    "\n",
    "    for date, job in zip(dates, job_entries):\n",
    "        job_title_match = re.search('|'.join(job_title_keywords), job, re.IGNORECASE)\n",
    "        job_title = job_title_match.group(0) if job_title_match else ''\n",
    "        company_match = re.search(company_patterns, job)\n",
    "        company = company_match.group(2) if company_match else ''\n",
    "        location_match = re.search(location_regex, job)\n",
    "        location = location_match.group(1) if location_match else ''\n",
    "        responsibilities = re.findall(r\"•\\s*.*|(?<=\\n)-\\s*.*\", job)\n",
    "        responsibilities_text = ' '.join(responsibilities).strip()\n",
    "\n",
    "        experiences.append({\n",
    "            \"Company\": company,\n",
    "            \"Job Title\": job_title,\n",
    "            \"Location\": location,\n",
    "            \"Dates\": date,\n",
    "            \"Responsibilities\": responsibilities_text\n",
    "        })\n",
    "\n",
    "    return experiences\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_education(text):\n",
    "    education_list = []\n",
    "    \n",
    "    # Define regex patterns for degrees, universities, schools, CGPA/SGPA/percentage, and dates\n",
    "    degree_patterns = [\n",
    "        r\"(Bachelor|Master|Associate|Doctorate|Ph\\.?D|Diploma|Certificate)\\s*(of\\s*(Arts|Science|Business|Engineering|Technology|Management|Education|Commerce|Law|Medicine|Fine Arts|Social Work|Computer Science|Psychology|Nursing|Physics|Chemistry|Biology))?\",\n",
    "        r\"(BA|BS|BSc|MA|MS|MBA|MFA|LLB|LLM|PhD|M\\.Ed|B\\.Ed|BBA)\"\n",
    "    ]\n",
    "    university_patterns = r\"[A-Z][a-zA-Z\\s&]*\\s*(University|Institute|College|Academy|Polytechnic)\"\n",
    "    school_patterns = r\"[A-Z][a-zA-Z\\s&]*\\s*(High School|Secondary School|Senior School|School)\"\n",
    "    time_period_pattern = r\"((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\s*[-–to]+\\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?[a-z]*\\s*\\d{4})\"\n",
    "    year_pattern = r\"\\b(19|20)\\d{2}\\b\"\n",
    "    score_patterns = [\n",
    "        r\"(CGPA|SGPA)\\s*[:\\-]?\\s*(\\d\\.\\d{1,2})\",  # CGPA/SGPA with possible decimals\n",
    "        r\"(\\d{1,2}\\.\\d{1,2})\\s*\\/\\s*10\",  # e.g., \"8.5/10\"\n",
    "        r\"(\\d{1,2})\\s*%\",  # Percentage\n",
    "    ]\n",
    "    \n",
    "    # Find the lines that might contain education details\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Extract degree details\n",
    "        degree_match = None\n",
    "        for pattern in degree_patterns:\n",
    "            degree_match = re.search(pattern, line, re.IGNORECASE)\n",
    "            if degree_match:\n",
    "                degree = degree_match.group(0)\n",
    "                break\n",
    "        degree = degree if degree_match else \"Not Found\"\n",
    "        \n",
    "        # Extract university details\n",
    "        university_match = re.search(university_patterns, line, re.IGNORECASE)\n",
    "        university = university_match.group(0) if university_match else \"Not Found\"\n",
    "        \n",
    "        # Extract school details if no university found\n",
    "        school_match = re.search(school_patterns, line, re.IGNORECASE)\n",
    "        school = school_match.group(0) if school_match else \"Not Found\"\n",
    "        \n",
    "        # Extract time period (e.g., \"August 2016 - May 2020\") or standalone year\n",
    "        time_period_match = re.search(time_period_pattern, line, re.IGNORECASE)\n",
    "        time_period = time_period_match.group(0) if time_period_match else \"Not Found\"\n",
    "        \n",
    "        # Extract standalone year if no time period was found\n",
    "        if time_period == \"Not Found\":\n",
    "            year_match = re.search(year_pattern, line)\n",
    "            time_period = year_match.group(0) if year_match else \"Not Found\"\n",
    "        \n",
    "        # Extract CGPA, SGPA, or percentage if present\n",
    "        score = \"Not Found\"\n",
    "        for pattern in score_patterns:\n",
    "            score_match = re.search(pattern, line, re.IGNORECASE)\n",
    "            if score_match:\n",
    "                score = score_match.group(0)\n",
    "                break\n",
    "        \n",
    "        # Only add an entry if it contains more than just \"Not Found\"\n",
    "        if degree != \"Not Found\" or university != \"Not Found\" or school != \"Not Found\" or time_period != \"Not Found\" or score != \"Not Found\":\n",
    "            education_list.append({\n",
    "                \"Degree\": degree,\n",
    "                \"University\": university,\n",
    "                \"School\": school,\n",
    "                \"Year/Time Period\": time_period,\n",
    "                \"Score\": score\n",
    "            })\n",
    "    \n",
    "    return education_list\n",
    "\n",
    "\n",
    "# Function to extract languages\n",
    "def extract_languages(text):\n",
    "    language_keywords = [\"languages\", \"skills\", \"proficiency\"]\n",
    "    for keyword in language_keywords:\n",
    "        language_start = text.find(keyword)\n",
    "        if language_start != -1:\n",
    "            lines = text.splitlines()[language_start:]\n",
    "            languages = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                language_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if language_match:\n",
    "                    languages.append(language_match.group())\n",
    "            return languages\n",
    "    return []\n",
    "\n",
    "# Function to extract certificates\n",
    "def extract_certificates(text):\n",
    "    certificate_keywords = [\"certifications\", \"accreditations\"]\n",
    "    for keyword in certificate_keywords:\n",
    "        certificate_start = text.find(keyword)\n",
    "        if certificate_start != -1:\n",
    "            lines = text.splitlines()[certificate_start:]\n",
    "            certificates = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                certificate_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if certificate_match:\n",
    "                    certificates.append(certificate_match.group())\n",
    "            return certificates\n",
    "    return []\n",
    "def extract_skills(text):\n",
    "    # Predefined list of common technical and soft skills\n",
    "    skills_list = [\n",
    "        \"C\", \"C++\", \"Python\", \"Java\", \"SQL\", \"JavaScript\", \"HTML\", \"CSS\", \"React\", \"Angular\",\n",
    "        \"Machine Learning\", \"Deep Learning\", \"Data Structures\", \"Data Analysis\", \"Project Management\",\n",
    "        \"Leadership\", \"Communication\", \"Teamwork\", \"Problem Solving\", \"Critical Thinking\",\n",
    "        \"Agile\", \"Scrum\", \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"DevOps\", \"Data Science\",\n",
    "        \"R\", \"Excel\", \"Power BI\", \"Tableau\", \"Automation\", \"Selenium\", \"REST APIs\", \n",
    "        \"Networking\", \"Cybersecurity\", \"Cloud Computing\", \"Blockchain\", \"UI/UX Design\",\n",
    "        \"Customer Service\", \"Salesforce\", \"SAP\", \"Digital Marketing\", \"SEO\", \"Content Writing\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the skills list to lowercase for case-insensitive matching\n",
    "    skills_set = set(skill.lower() for skill in skills_list)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Extract the skills by checking if any word in the text matches the skills list\n",
    "    extracted_skills = [word for word in words if word.lower() in skills_set]\n",
    "    \n",
    "    # Extract skill lines that might include proficiency symbols (e.g., \"○ ○ ○ ○ ○\")\n",
    "    proficiency_pattern = r\"([A-Za-z\\s,]+)\\s*([○●]{1,5})\"\n",
    "    proficiency_matches = re.findall(proficiency_pattern, text)\n",
    "    \n",
    "    # Add skills from proficiency matches\n",
    "    for match in proficiency_matches:\n",
    "        skill = match[0].strip()\n",
    "        if skill.lower() in skills_set:\n",
    "            extracted_skills.append(skill)\n",
    "\n",
    "    # Extract skills listed in a comma-separated format\n",
    "    skills_pattern = r\"([A-Za-z\\s\\+\\#\\&]+(?:,\\s*[A-Za-z\\s\\+\\#\\&]+)*)\"\n",
    "    skills_matches = re.findall(skills_pattern, text)\n",
    "    for match in skills_matches:\n",
    "        for skill in match.split(','):\n",
    "            cleaned_skill = skill.strip()\n",
    "            if cleaned_skill.lower() in skills_set:\n",
    "                extracted_skills.append(cleaned_skill)\n",
    "\n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    unique_skills = list(set(extracted_skills))\n",
    "    \n",
    "    return unique_skills if unique_skills else [\"Not Found\"]\n",
    "\n",
    "\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "def process_resumes(folder_path, output_csv_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            name = extract_name(text)\n",
    "            profile_summary = extract_profile_summary(text)\n",
    "            email = extract_email(text)\n",
    "            phone = extract_phone_number(text)\n",
    "            address = extract_address(text)\n",
    "            links = extract_links_from_pdf(pdf_path)  # Updated to use the new link extraction method\n",
    "            experience = extract_experience(text)\n",
    "            education = extract_education(text)\n",
    "            languages = extract_languages(text)\n",
    "            certificates = extract_certificates(text)\n",
    "            skills = extract_skills(text)\n",
    "\n",
    "            data.append({\n",
    "                'File Name': filename,\n",
    "                'Name': name,\n",
    "                'Profile Summary': profile_summary,\n",
    "                'Email': email,\n",
    "                'Phone': phone,\n",
    "                'Address': address,\n",
    "                'Links': links,\n",
    "                'Experience': experience,\n",
    "                'Education': education,\n",
    "                'Languages': languages,\n",
    "                'Certificates': certificates,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if os.path.exists(output_csv_path):\n",
    "        os.remove(output_csv_path)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Data saved to {output_csv_path}')\n",
    "\n",
    "\n",
    "process_resumes('C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\\\\bb', 'chatgpt1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9beb93b-9fe5-4513-b789-05a7763527b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
