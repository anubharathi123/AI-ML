{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d6d958e-9dd5-4c18-9f12-1d69a270b2b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 2.9/12.8 MB 18.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.3/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282b697e-1cda-4e5b-8a88-f9d46b31d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e8af50-26b8-447f-b750-57de1fa25dab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santhoshs.s\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\santhoshs.s\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\santhoshs.s\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373d89da-bbca-426f-ad92-a2e1e9eaecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(resume_text):\n",
    "  \"\"\"Extracts information from resume text using spaCy.\"\"\"\n",
    "\n",
    "  doc = nlp(resume_text)\n",
    "\n",
    "  name = None\n",
    "  email = None\n",
    "  phone = None\n",
    "  skills = []\n",
    "  experience = []\n",
    "  education = []\n",
    "\n",
    "  for entity in doc.ents:\n",
    "    if entity.label_ == \"PERSON\":\n",
    "      name = entity.text\n",
    "    elif entity.label_ == \"EMAIL\":\n",
    "      email = entity.text\n",
    "    elif entity.label_ == \"ORG\":\n",
    "      experience.append(entity.text)\n",
    "    elif entity.label_ == \"GPE\":\n",
    "      education.append(entity.text)\n",
    "\n",
    "  for token in doc:\n",
    "      if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "          if token.text.lower() in ['python','java','c++','sql']:\n",
    "              skills.append(token.text)\n",
    "  \n",
    "  # Extract phone number using regex\n",
    "  phone_match = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', resume_text)\n",
    "  if phone_match:\n",
    "    phone = phone_match[0]\n",
    "\n",
    "  return {\n",
    "      \"name\": name,\n",
    "      \"email\": email,\n",
    "      \"phone\": phone,\n",
    "      \"skills\": skills,\n",
    "      \"experience\": experience,\n",
    "      \"education\": education,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10a6291-7680-440e-b4f3-2055aa049869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91682a2f-44a1-46a7-93ce-68ae674673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_info_list = []\n",
    "def process_resumes_in_folder(folder_path):\n",
    "      # Initialize a list to store extracted information\n",
    "\n",
    "    # Your existing code to process resumes and extract information\n",
    "    # For example:\n",
    "    for resume_file in os.listdir(folder_path):\n",
    "        resume_path = os.path.join(folder_path, resume_file)\n",
    "        # Assuming you have a function to extract info from the resume\n",
    "        extracted_info = extract_information(resume_path)  # This should return a dictionary\n",
    "        if extracted_info:  # Check if extracted_info is not None or empty\n",
    "            extracted_info_list.append(extracted_info)  # Append the extracted info to the list\n",
    "\n",
    "    return extracted_info_list  # Return the list of extracted information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb7ef13-183a-4144-bba4-af666f77d8b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': ['C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\\\\31064969.pdf'],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []},\n",
       " {'name': None,\n",
       "  'email': None,\n",
       "  'phone': None,\n",
       "  'skills': [],\n",
       "  'experience': [],\n",
       "  'education': []}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\"\n",
    "process_resumes_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33457da6-8033-4e51-93be-f6aba96f69aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_to_csv(extracted_info_list, filename=\"extracted_resume_info.csv\"):\n",
    "    \"\"\"Saves the extracted information to a CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['name', 'email', 'phone', 'skills', 'experience', 'education']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for extracted_info in extracted_info_list:\n",
    "            writer.writerow(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a66ded8-b95e-43ff-a571-775fdc661d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(extracted_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe093ae1-277a-4b2d-8343-aeea6d75ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to output2.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "import re\n",
    "\n",
    "def extract_profile_summary(text):\n",
    "    # List of potential keywords indicating the profile section\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "\n",
    "    # Normalize keywords: Create regex patterns to ignore spaces\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    # Attempt to find the starting point of the summary based on keywords\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):  # Check for keyword with regex\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    # If a summary section was found, extract subsequent lines\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        \n",
    "        # Start extracting from the next line after the keyword\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # Stop extraction if the line is empty or contains bullet points, but allow for continuation\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue  # Allow extraction to continue past bullet points and empty lines\n",
    "            if line.isupper():  # Stop if we reach a fully uppercase line, which may indicate a section title\n",
    "                break\n",
    "            \n",
    "            # Add the line to the extracted lines\n",
    "            extracted_lines.append(line)\n",
    "\n",
    "        # Join the extracted lines into a single summary string\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    # Regular expression pattern for phone numbers, allowing symbols like (), -, and spaces\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. ()]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    \n",
    "    # If a match is found, remove all non-digit characters to get a clean number\n",
    "    if match:\n",
    "        # Combine all groups into a single string and remove any non-numeric characters\n",
    "        raw_number = match.group(0)\n",
    "        clean_number = re.sub(r'\\D', '', raw_number)  # Remove all non-digit characters\n",
    "        return clean_number\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "\n",
    "# Function to extract address without including the keyword \"address\"\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Iterate through the lines to find keywords\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Check if any of the address keywords are in the line\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            # Extract lines after the keyword to capture the full address\n",
    "            extracted_lines = []\n",
    "            \n",
    "            # If the keyword \"address\" is present in the line, ignore it and start from the next part of the line\n",
    "            clean_line = re.sub(r'\\b(?:' + '|'.join(address_keywords) + r')\\b', '', line, flags=re.IGNORECASE).strip()\n",
    "            if clean_line:\n",
    "                extracted_lines.append(clean_line)\n",
    "            \n",
    "            # Attempt to include a few subsequent lines in case the address continues\n",
    "            for i in range(1, 3):\n",
    "                if idx + i < len(lines):\n",
    "                    next_line = lines[idx + i].strip()\n",
    "                    if next_line:\n",
    "                        extracted_lines.append(next_line)\n",
    "            \n",
    "            # Combine the extracted lines into a single string\n",
    "            return \" \".join(extracted_lines).strip()\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "import re\n",
    "def extract_links(text):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from text, including standard URLs, HTML-like links, and markdown links.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text from which to extract links.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted hyperlinks or \"Not Found\" if no links are found.\n",
    "    \"\"\"\n",
    "    # Standard URLs\n",
    "    link_pattern = r'https?://[^\\s]+'\n",
    "    \n",
    "    # HTML links\n",
    "    html_link_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']'\n",
    "    \n",
    "    # Markdown links\n",
    "    markdown_link_pattern = r'\\[.*?\\]\\((https?://[^\\s]+)\\)'\n",
    "    \n",
    "    # Plain links like www.example.com or example.com\n",
    "    plain_link_pattern = r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "    \n",
    "    # Find all matches for each pattern\n",
    "    standard_links = re.findall(link_pattern, text)\n",
    "    html_links = re.findall(html_link_pattern, text)\n",
    "    markdown_links = re.findall(markdown_link_pattern, text)\n",
    "    plain_links = re.findall(plain_link_pattern, text)\n",
    "    \n",
    "    # Combine all found links into a single list\n",
    "    hyperlinks = standard_links + html_links + markdown_links + plain_links\n",
    "    \n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_links = list(set(hyperlinks))\n",
    "    \n",
    "    return unique_links if unique_links else [\"Not Found\"]\n",
    "\n",
    "def extract_experience(text):\n",
    "    \"\"\"\n",
    "    Extracts work experience from the resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract work experience.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing job title, company, location, dates, and responsibilities.\n",
    "    \"\"\"\n",
    "    experience_list = []\n",
    "    \n",
    "    # Improved regex to capture common experience formats\n",
    "    experience_pattern = r'(?i)(.+?)(?:\\s*-\\s*|\\s*\\(?\\s*(?:\\w+\\s*)+\\s*\\)?)\\s*(.+?)\\s*[,|]\\s*(.+?)\\s*[-–]?\\s*(\\d{4}[-–]?\\d{0,4})?\\s*[-–]?\\s*(.*?)(?=\\n\\s*\\n|\\Z)'\n",
    "\n",
    "    # Find all matches\n",
    "    matches = re.findall(experience_pattern, text, re.DOTALL)\n",
    "\n",
    "    for match in matches:\n",
    "        job_title = match[0].strip() if match[0] else \"Not Found\"\n",
    "        company = match[1].strip() if match[1] else \"Not Found\"\n",
    "        location = match[2].strip() if match[2] else \"Not Found\"\n",
    "        employment_dates = match[3].strip() if match[3] else \"Not Found\"\n",
    "        responsibilities = match[4].strip().replace('\\n', ' ') if match[4] else \"Not Found\"\n",
    "\n",
    "        experience_list.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company': company,\n",
    "            'Location': location,\n",
    "            'Employment Dates': employment_dates,\n",
    "            'Responsibilities': responsibilities\n",
    "        })\n",
    "\n",
    "    return experience_list if experience_list else [{\"Experience\": \"Not Found\"}]\n",
    "\n",
    "# Function to extract education details\n",
    "def extract_education(text):\n",
    "    education_list = []\n",
    "    education_keywords = [\"education\", \"academic background\"]\n",
    "    for keyword in education_keywords:\n",
    "        education_start = text.find(keyword)\n",
    "        if education_start != -1:\n",
    "            lines = text.splitlines()[education_start:]  # Text after education section\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue  # Skip empty lines\n",
    "                university_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if university_match:\n",
    "                    university = university_match.group()\n",
    "                    degree_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                    degree = degree_match.group() if degree_match else \"\"\n",
    "                    year_match = re.search(r\"\\d{4}\", line)\n",
    "                    year = year_match.group() if year_match else \"\"\n",
    "                    education_list.append({\n",
    "                        \"University\": university,\n",
    "                        \"Degree\": degree,\n",
    "                        \"Year\": year\n",
    "                    })\n",
    "            break\n",
    "    return education_list\n",
    "\n",
    "# Function to extract languages\n",
    "def extract_languages(text):\n",
    "    language_keywords = [\"languages\", \"skills\", \"proficiency\"]\n",
    "    for keyword in language_keywords:\n",
    "        language_start = text.find(keyword)\n",
    "        if language_start != -1:\n",
    "            lines = text.splitlines()[language_start:]\n",
    "            languages = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                language_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if language_match:\n",
    "                    languages.append(language_match.group())\n",
    "            return languages\n",
    "    return []\n",
    "\n",
    "# Function to extract certificates\n",
    "def extract_certificates(text):\n",
    "    certificate_keywords = [\"certifications\", \"accreditations\"]\n",
    "    for keyword in certificate_keywords:\n",
    "        certificate_start = text.find(keyword)\n",
    "        if certificate_start != -1:\n",
    "            lines = text.splitlines()[certificate_start:]\n",
    "            certificates = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                certificate_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if certificate_match:\n",
    "                    certificates.append(certificate_match.group())\n",
    "            return certificates\n",
    "    return []\n",
    "\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "def process_resumes(folder_path, output_csv_path):\n",
    "    # Create an empty DataFrame to hold new data\n",
    "    data = []\n",
    "\n",
    "    # Loop through all files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Extract information (profile, name, email, phone, address, links, experience, education, languages, certificates)\n",
    "            name = extract_name(text)\n",
    "            email = extract_email(text)\n",
    "            phone = extract_phone_number(text)\n",
    "            address = extract_address(text)\n",
    "            links = extract_links(text)  # Use the modified links extraction function\n",
    "            experience = extract_experience(text)\n",
    "            education = extract_education(text)\n",
    "            languages = extract_languages(text)\n",
    "            certificates = extract_certificates(text)\n",
    "            profile_summary = extract_profile_summary(text)\n",
    "\n",
    "            # Add extracted info to data list\n",
    "            data.append({\n",
    "                'File Name': filename,\n",
    "                'Name': name,\n",
    "                'Email': email,\n",
    "                'Phone': phone,\n",
    "                'Address': address,\n",
    "                'Links': ', '.join(links),  # Convert list of links to a comma-separated string\n",
    "                'Experience': experience,\n",
    "                'Education': education,\n",
    "                'Languages': languages,\n",
    "                'Certificates': certificates,\n",
    "                'Profile Summary': profile_summary\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Clear previous data in the CSV file\n",
    "    if os.path.exists(output_csv_path):\n",
    "        os.remove(output_csv_path)  # Remove the existing file\n",
    "\n",
    "    # Save the new data to the CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Data saved to {output_csv_path}')\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'path_to_resume_folder' with the actual folder path containing PDF resumes\n",
    "# Replace 'output.csv' with your desired output file name\n",
    "process_resumes('C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\\\\bb', 'output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef22ee9-e467-4792-8f7d-ae94ecf0bfc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample testing code\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample resume text for testing\n",
    "    sample_text = \"\"\"\n",
    "    Work Experience\n",
    "\n",
    "    Nutrition Consultant - WIC (Port Washington) 2021 - Present\n",
    "    - Provided nutrition education counseling on morbid obesity, high cholesterol, and diabetes.\n",
    "\n",
    "    Nutrition Consultant - DaVita (Delmas) 2016 - 2019\n",
    "    - Registered nutrition consultant/educator responsible for educating, counseling, and supporting patients.\n",
    "\n",
    "    Education\n",
    "    Master of Science in Dietary Education, Golden Valley University\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_experience = extract_experience(sample_text)\n",
    "    print(extracted_experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8585efc-4a94-420a-826c-93806479f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to output4.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "def extract_profile_summary(text):\n",
    "    # List of potential keywords indicating the profile section\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "\n",
    "    # Normalize keywords: Create regex patterns to ignore spaces\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    # Attempt to find the starting point of the summary based on keywords\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):  # Check for keyword with regex\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    # If a summary section was found, extract subsequent lines\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        \n",
    "        # Start extracting from the next line after the keyword\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # Stop extraction if the line is empty or contains bullet points, but allow for continuation\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue  # Allow extraction to continue past bullet points and empty lines\n",
    "            if line.isupper():  # Stop if we reach a fully uppercase line, which may indicate a section title\n",
    "                break\n",
    "            \n",
    "            # Add the line to the extracted lines\n",
    "            extracted_lines.append(line)\n",
    "\n",
    "        # Join the extracted lines into a single summary string\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return match.group(0).replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\" \", \"\") if match else \"Not Found\"\n",
    "\n",
    "# Function to extract address without including the keyword \"address\"\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Iterate through the lines to find keywords\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Check if any of the address keywords are in the line\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            # Extract lines after the keyword to capture the full address\n",
    "            extracted_lines = []\n",
    "            \n",
    "            # If the keyword \"address\" is present in the line, ignore it and start from the next part of the line\n",
    "            clean_line = re.sub(r'\\b(?:' + '|'.join(address_keywords) + r')\\b', '', line, flags=re.IGNORECASE).strip()\n",
    "            if clean_line:\n",
    "                extracted_lines.append(clean_line)\n",
    "            \n",
    "            # Attempt to include a few subsequent lines in case the address continues\n",
    "            for i in range(1, 3):\n",
    "                if idx + i < len(lines):\n",
    "                    next_line = lines[idx + i].strip()\n",
    "                    if next_line:\n",
    "                        extracted_lines.append(next_line)\n",
    "            \n",
    "            # Combine the extracted lines into a single string\n",
    "            return \" \".join(extracted_lines).strip()\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract links\n",
    "def extract_links(text):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from text, including standard URLs, HTML-like links, and markdown links.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text from which to extract links.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted hyperlinks or \"Not Found\" if no links are found.\n",
    "    \"\"\"\n",
    "    # Standard URLs\n",
    "    link_pattern = r'https?://[^\\s]+'\n",
    "    \n",
    "    # HTML links\n",
    "    html_link_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']'\n",
    "    \n",
    "    # Markdown links\n",
    "    markdown_link_pattern = r'\\[.*?\\]\\((https?://[^\\s]+)\\)'\n",
    "    \n",
    "    # Plain links like www.example.com or example.com\n",
    "    plain_link_pattern = r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "    \n",
    "    # Find all matches for each pattern\n",
    "    standard_links = re.findall(link_pattern, text)\n",
    "    html_links = re.findall(html_link_pattern, text)\n",
    "    markdown_links = re.findall(markdown_link_pattern, text)\n",
    "    plain_links = re.findall(plain_link_pattern, text)\n",
    "    \n",
    "    # Combine all found links into a single list\n",
    "    hyperlinks = standard_links + html_links + markdown_links + plain_links\n",
    "    \n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_links = list(set(hyperlinks))\n",
    "    \n",
    "    return unique_links if unique_links else [\"Not Found\"]\n",
    "\n",
    "# Function to extract experience details\n",
    "def extract_experience(text):\n",
    "    experience_list = []\n",
    "\n",
    "    # List of keywords that indicate the start of an experience section\n",
    "    experience_keywords = [\n",
    "        \"work history\", \"employment history\", \"work experience\", \"employment experience\",\n",
    "        \"employment\", \"professional experience\", \"professional background\",\n",
    "        \"military background\", \"military experience\", \"internships\",\n",
    "        \"paid internships\", \"work internships\", \"professional internships\",\n",
    "        \"employment background\"\n",
    "    ]\n",
    "\n",
    "    # Create a regex pattern for the keywords (case insensitive)\n",
    "    keywords_pattern = r'(?i)(' + '|'.join(experience_keywords) + r')'\n",
    "\n",
    "    # Find the starting point for experience extraction\n",
    "    experience_start = re.search(keywords_pattern, text)\n",
    "    if experience_start:\n",
    "        # Extract text starting from the found keyword\n",
    "        experience_section = text[experience_start.end():]\n",
    "\n",
    "        # Improved regex pattern to capture job title, company, location, dates, and responsibilities\n",
    "        experience_pattern = r'(?i)(?P<job_title>.+?)(?:\\s+at\\s+|\\s*-*\\s*)(?P<company>[^\\n,]+?)\\s*,?\\s*(?P<location>[^\\n,]+?)\\s*[-–]?\\s*(?P<dates>[\\w\\s]+(?:[-–]\\s*[\\w\\s]+)?)?\\s*(?P<responsibilities>.+?)(?=\\n\\s*\\n|\\Z)'\n",
    "\n",
    "        # Find all matches in the extracted experience section\n",
    "        matches = re.finditer(experience_pattern, experience_section, re.DOTALL)\n",
    "\n",
    "        for match in matches:\n",
    "            job_title = match.group(\"job_title\").strip() if match.group(\"job_title\") else \"Not Found\"\n",
    "            company = match.group(\"company\").strip() if match.group(\"company\") else \"Not Found\"\n",
    "            location = match.group(\"location\").strip() if match.group(\"location\") else \"Not Found\"\n",
    "            employment_dates = match.group(\"dates\").strip() if match.group(\"dates\") else \"Not Found\"\n",
    "            responsibilities = match.group(\"responsibilities\").strip().replace('\\n', ' ') if match.group(\"responsibilities\") else \"Not Found\"\n",
    "\n",
    "            experience_list.append({\n",
    "                'Job Title': job_title,\n",
    "                'Company': company,\n",
    "                'Location': location,\n",
    "                'Employment Dates': employment_dates,\n",
    "                'Responsibilities': responsibilities\n",
    "            })\n",
    "\n",
    "    return experience_list if experience_list else [{\"Experience\": \"Not Found\"}]\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract education details\n",
    "def extract_education(text):\n",
    "    education_list = []\n",
    "    education_keywords = [\"education\", \"academic background\"]\n",
    "    for keyword in education_keywords:\n",
    "        education_start = text.find(keyword)\n",
    "        if education_start != -1:\n",
    "            lines = text.splitlines()[education_start:]  # Text after education section\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue  # Skip empty lines\n",
    "                university_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if university_match:\n",
    "                    university = university_match.group()\n",
    "                    degree_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                    degree = degree_match.group() if degree_match else \"\"\n",
    "                    year_match = re.search(r\"\\d{4}\", line)\n",
    "                    year = year_match.group() if year_match else \"\"\n",
    "                    education_list.append({\n",
    "                        \"University\": university,\n",
    "                        \"Degree\": degree,\n",
    "                        \"Year\": year\n",
    "                    })\n",
    "            break\n",
    "    return education_list\n",
    "\n",
    "# Function to extract languages\n",
    "def extract_languages(text):\n",
    "    language_keywords = [\"languages\", \"skills\", \"proficiency\"]\n",
    "    for keyword in language_keywords:\n",
    "        language_start = text.find(keyword)\n",
    "        if language_start != -1:\n",
    "            lines = text.splitlines()[language_start:]\n",
    "            languages = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                language_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if language_match:\n",
    "                    languages.append(language_match.group())\n",
    "            return languages\n",
    "    return []\n",
    "\n",
    "# Function to extract certificates\n",
    "def extract_certificates(text):\n",
    "    certificate_keywords = [\"certifications\", \"accreditations\"]\n",
    "    for keyword in certificate_keywords:\n",
    "        certificate_start = text.find(keyword)\n",
    "        if certificate_start != -1:\n",
    "            lines = text.splitlines()[certificate_start:]\n",
    "            certificates = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                certificate_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if certificate_match:\n",
    "                    certificates.append(certificate_match.group())\n",
    "            return certificates\n",
    "    return []\n",
    "\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "def process_resumes(folder_path, output_csv_path):\n",
    "    # Create an empty DataFrame to hold new data\n",
    "    data = []\n",
    "\n",
    "    # Loop through all files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            # Extract information (profile, name, email, phone, address, links, experience, education, languages, certificates)\n",
    "            name = extract_name(text)\n",
    "            profile_summary = extract_profile_summary(text)\n",
    "            email = extract_email(text)\n",
    "            phone = extract_phone_number(text)\n",
    "            address = extract_address(text)\n",
    "            links = extract_links(text)\n",
    "            experience = extract_experience(text)\n",
    "            education = extract_education(text)\n",
    "            languages = extract_languages(text)\n",
    "            certificates = extract_certificates(text)\n",
    "\n",
    "            # Append extracted data to the list\n",
    "            data.append({\n",
    "                'Name': name,\n",
    "                'Profile Summary': profile_summary,\n",
    "                'Email': email,\n",
    "                'Phone': phone,\n",
    "                'Address': address,\n",
    "                'Links': links,\n",
    "                'Experience': experience,\n",
    "                'Education': education,\n",
    "                'Languages': languages,\n",
    "                'Certificates': certificates\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Data saved to {output_csv_path}')\n",
    "\n",
    "# Example usage\n",
    "process_resumes('C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\\\\bb', 'output4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdb5f3e-3441-45fd-9dac-e8118276a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\santhoshs.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\santhoshs.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\santhoshs.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytesseract) (10.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e256d46b-6815-4e49-a6ee-39434e5680ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "C:\\\\Users\\\\santhoshs.s\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\tesseract.exe is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytesseract\\pytesseract.py:275\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubprocess_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Perform OCR on the image\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(extracted_text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytesseract\\pytesseract.py:486\u001b[0m, in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytesseract\\pytesseract.py:489\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m--> 489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytesseract\\pytesseract.py:352\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[0;32m    342\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    350\u001b[0m     }\n\u001b[1;32m--> 352\u001b[0m     \u001b[43mrun_tesseract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    355\u001b[0m         return_bytes,\n\u001b[0;32m    356\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytesseract\\pytesseract.py:280\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m: C:\\\\Users\\\\santhoshs.s\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\tesseract.exe is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Specify the path to your Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Users\\\\santhoshs.s\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Scripts\\\\tesseract.exe'  # Adjust this path if needed\n",
    "\n",
    "# Load the image\n",
    "image_path = 'C:\\\\Users\\\\santhoshs.s\\\\Pictures\\\\ss.png'  # Update with your image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Perform OCR on the image\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22930cbe-8747-409d-9532-aef49e7eb7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
