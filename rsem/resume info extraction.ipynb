{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c857be-f12e-49b8-b038-8297cbe5e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to chatgpt.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "def extract_profile_summary(text):\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\", \"Profile\"\n",
    "    ]\n",
    "    \n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.isupper():\n",
    "                break\n",
    "            extracted_lines.append(line)\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    # Regular expression pattern for phone numbers, allowing symbols like (), -, and spaces\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. ()]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    \n",
    "    # If a match is found, remove all non-digit characters to get a clean number\n",
    "    if match:\n",
    "        # Combine all groups into a single string and remove any non-numeric characters\n",
    "        raw_number = match.group(0)\n",
    "        clean_number = re.sub(r'\\D', '', raw_number)  # Remove all non-digit characters\n",
    "        return clean_number\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract address\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            return line.strip()\n",
    "    return \"Not Found\"\n",
    "\n",
    "def extract_links(text):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from text, including standard URLs, HTML-like links, and markdown links, and saves them in separate columns based on their type.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to extract links.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing separate lists for LinkedIn, GitHub, email, Kaggle, portfolio, Facebook, Instagram, Git, Monster, and other links.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard URLs\n",
    "    standard_link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "    # LinkedIn links\n",
    "    linkedin_link_pattern = r'https?://www\\.linkedin\\.com/in/[a-zA-Z0-9-]*'\n",
    "\n",
    "    # GitHub links\n",
    "    github_link_pattern = r'https?://github\\.com/[a-zA-Z0-9-]*'\n",
    "\n",
    "    # Email links\n",
    "    email_link_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "    # Kaggle links\n",
    "    kaggle_link_pattern = r'https?://www\\.kaggle\\.com/([a-zA-Z0-9-]*|profile/[a-zA-Z0-9-]*)'\n",
    "\n",
    "    # Portfolio links\n",
    "    portfolio_link_pattern = r'https?://(www\\.)?[a-zA-Z0-9-]*\\.(com|net|org|io|in)/?'\n",
    "\n",
    "    # Facebook links\n",
    "    facebook_link_pattern = r'https?://www\\.facebook\\.com/([a-zA-Z0-9-]*|profile\\.php\\?id=[0-9]+)'\n",
    "\n",
    "    # Instagram links\n",
    "    instagram_link_pattern = r'https?://www\\.instagram\\.com/([a-zA-Z0-9_]*)'\n",
    "\n",
    "    # Git links\n",
    "    git_link_pattern = r'https?://(git|gitlab|bitbucket)\\.(com|org)/[a-zA-Z0-9-]*/[a-zA-Z0-9-]*'\n",
    "\n",
    "    # Monster links\n",
    "    monster_link_pattern = r'https?://www\\.monster\\.com/jobs/search/?'\n",
    "\n",
    "    # HTML links\n",
    "    html_link_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']'\n",
    "\n",
    "    # Markdown links\n",
    "    markdown_link_pattern = r'\\[.*?\\]\\((https?://[^\\s]+)\\)'\n",
    "\n",
    "    # Plain links like www.example.com or example.com\n",
    "    plain_link_pattern = r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "\n",
    "    # Find all matches for each pattern\n",
    "    standard_links = re.findall(standard_link_pattern, text)\n",
    "    linkedin_links = re.findall(linkedin_link_pattern, text)\n",
    "    github_links = re.findall(github_link_pattern, text)\n",
    "    email_links = re.findall(email_link_pattern, text)\n",
    "    kaggle_links = re.findall(kaggle_link_pattern, text)\n",
    "    portfolio_links = re.findall(portfolio_link_pattern, text)\n",
    "    facebook_links = re.findall(facebook_link_pattern, text)\n",
    "    instagram_links = re.findall(instagram_link_pattern, text)\n",
    "    git_links = re.findall(git_link_pattern, text)\n",
    "    monster_links = re.findall(monster_link_pattern, text)\n",
    "    html_links = re.findall(html_link_pattern, text)\n",
    "    markdown_links = re.findall(markdown_link_pattern, text)\n",
    "    plain_links = re.findall(plain_link_pattern, text)\n",
    "\n",
    "    # Combine all found links into a single list\n",
    "    hyperlinks = standard_links + html_links + markdown_links + plain_links\n",
    "\n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_links = list(set(hyperlinks))\n",
    "\n",
    "    # Create a dictionary to store links by type\n",
    "    links_dict = {\n",
    "        'LinkedIn': linkedin_links,\n",
    "        'GitHub': github_links,\n",
    "        'Email': email_links,\n",
    "        'Kaggle': kaggle_links,\n",
    "        'Portfolio': portfolio_links,\n",
    "        'Facebook': facebook_links,\n",
    "        'Instagram': instagram_links,\n",
    "        'Git': git_links,\n",
    "        'Monster': monster_links,\n",
    "        'Other': [link for link in unique_links if link not in linkedin_links and link not in github_links and link not in email_links and link not in kaggle_links and link not in portfolio_links and link not in facebook_links and link not in instagram_links and link not in git_links and link not in monster_links]\n",
    "    }\n",
    "\n",
    "    return links_dict\n",
    "\n",
    "# Updated Function to extract experience details\n",
    "def extract_experience(text):\n",
    "    experience_keywords = [\n",
    "        \"work history\", \"employment history\", \"work experience\",\n",
    "        \"professional experience\", \"career summary\", \"professional background\",\n",
    "        \"job experience\", \"internships\", \"relevant experience\",\n",
    "        \"contract work\", \"military experience\", \"volunteer work\"\n",
    "    ]\n",
    "    \n",
    "    date_patterns = [\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\",\n",
    "        r\"\\b\\d{4}\\s*[-–—to]+\\s*\\d{4}\\b\",\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*Present\",\n",
    "        r\"\\b\\d{4}\\s*[-–—to]+\\s*Present\\b\"\n",
    "    ]\n",
    "\n",
    "    job_title_keywords = [\n",
    "        \"manager\", \"director\", \"engineer\", \"analyst\", \"consultant\",\n",
    "        \"assistant\", \"coordinator\", \"specialist\", \"developer\", \"designer\",\n",
    "        \"executive\", \"advisor\", \"technician\", \"officer\", \"intern\", \"trainee\"\n",
    "    ]\n",
    "\n",
    "    company_patterns = r\"(at|with|for)\\s+([A-Z][\\w&,. ]+)\"\n",
    "    location_patterns = [\n",
    "        r\"\\bin\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b\",\n",
    "        r\"\\b([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\s+(USA|Inc|Ltd|LLC|Co)\\b\"\n",
    "    ]\n",
    "\n",
    "    location_regex = '|'.join(location_patterns)\n",
    "    experience_start = re.search('|'.join(experience_keywords), text, re.IGNORECASE)\n",
    "    if not experience_start:\n",
    "        return []\n",
    "\n",
    "    content = text[experience_start.start():]\n",
    "    dates = re.findall('|'.join(date_patterns), content)\n",
    "    job_entries = re.split('|'.join(date_patterns), content)\n",
    "    experiences = []\n",
    "\n",
    "    for date, job in zip(dates, job_entries):\n",
    "        job_title_match = re.search('|'.join(job_title_keywords), job, re.IGNORECASE)\n",
    "        job_title = job_title_match.group(0) if job_title_match else ''\n",
    "        company_match = re.search(company_patterns, job)\n",
    "        company = company_match.group(2) if company_match else ''\n",
    "        location_match = re.search(location_regex, job)\n",
    "        location = location_match.group(1) if location_match else ''\n",
    "        responsibilities = re.findall(r\"•\\s*.*|(?<=\\n)-\\s*.*\", job)\n",
    "        responsibilities_text = ' '.join(responsibilities).strip()\n",
    "\n",
    "        experiences.append({\n",
    "            \"Company\": company,\n",
    "            \"Job Title\": job_title,\n",
    "            \"Location\": location,\n",
    "            \"Dates\": date,\n",
    "            \"Responsibilities\": responsibilities_text\n",
    "        })\n",
    "\n",
    "    return experiences\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_education(text):\n",
    "    education_list = []\n",
    "    \n",
    "    # Define regex patterns for degrees, universities, schools, CGPA/SGPA/percentage, and dates\n",
    "    degree_patterns = [\n",
    "        r\"(Bachelor|Master|Associate|Doctorate|Ph\\.?D|Diploma|Certificate)\\s*(of\\s*(Arts|Science|Business|Engineering|Technology|Management|Education|Commerce|Law|Medicine|Fine Arts|Social Work|Computer Science|Psychology|Nursing|Physics|Chemistry|Biology))?\",\n",
    "        r\"(BA|BS|BSc|MA|MS|MBA|MFA|LLB|LLM|PhD|M\\.Ed|B\\.Ed|BBA)\"\n",
    "    ]\n",
    "    university_patterns = r\"[A-Z][a-zA-Z\\s&]*\\s*(University|Institute|College|Academy|Polytechnic)\"\n",
    "    school_patterns = r\"[A-Z][a-zA-Z\\s&]*\\s*(High School|Secondary School|Senior School|School)\"\n",
    "    time_period_pattern = r\"((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\s*[-–to]+\\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?[a-z]*\\s*\\d{4})\"\n",
    "    year_pattern = r\"\\b(19|20)\\d{2}\\b\"\n",
    "    score_patterns = [\n",
    "        r\"(CGPA|SGPA)\\s*[:\\-]?\\s*(\\d\\.\\d{1,2})\",  # CGPA/SGPA with possible decimals\n",
    "        r\"(\\d{1,2}\\.\\d{1,2})\\s*\\/\\s*10\",  # e.g., \"8.5/10\"\n",
    "        r\"(\\d{1,2})\\s*%\",  # Percentage\n",
    "    ]\n",
    "    \n",
    "    # Find the lines that might contain education details\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Extract degree details\n",
    "        degree_match = None\n",
    "        for pattern in degree_patterns:\n",
    "            degree_match = re.search(pattern, line, re.IGNORECASE)\n",
    "            if degree_match:\n",
    "                degree = degree_match.group(0)\n",
    "                break\n",
    "        degree = degree if degree_match else \"Not Found\"\n",
    "        \n",
    "        # Extract university details\n",
    "        university_match = re.search(university_patterns, line, re.IGNORECASE)\n",
    "        university = university_match.group(0) if university_match else \"Not Found\"\n",
    "        \n",
    "        # Extract school details if no university found\n",
    "        school_match = re.search(school_patterns, line, re.IGNORECASE)\n",
    "        school = school_match.group(0) if school_match else \"Not Found\"\n",
    "        \n",
    "        # Extract time period (e.g., \"August 2016 - May 2020\") or standalone year\n",
    "        time_period_match = re.search(time_period_pattern, line, re.IGNORECASE)\n",
    "        time_period = time_period_match.group(0) if time_period_match else \"Not Found\"\n",
    "        \n",
    "        # Extract standalone year if no time period was found\n",
    "        if time_period == \"Not Found\":\n",
    "            year_match = re.search(year_pattern, line)\n",
    "            time_period = year_match.group(0) if year_match else \"Not Found\"\n",
    "        \n",
    "        # Extract CGPA, SGPA, or percentage if present\n",
    "        score = \"Not Found\"\n",
    "        for pattern in score_patterns:\n",
    "            score_match = re.search(pattern, line, re.IGNORECASE)\n",
    "            if score_match:\n",
    "                score = score_match.group(0)\n",
    "                break\n",
    "        \n",
    "        # Only add an entry if it contains more than just \"Not Found\"\n",
    "        if degree != \"Not Found\" or university != \"Not Found\" or school != \"Not Found\" or time_period != \"Not Found\" or score != \"Not Found\":\n",
    "            education_list.append({\n",
    "                \"Degree\": degree,\n",
    "                \"University\": university,\n",
    "                \"School\": school,\n",
    "                \"Year/Time Period\": time_period,\n",
    "                \"Score\": score\n",
    "            })\n",
    "    \n",
    "    return education_list\n",
    "\n",
    "\n",
    "# Function to extract languages\n",
    "def extract_languages(text):\n",
    "    language_keywords = [\"languages\", \"skills\", \"proficiency\"]\n",
    "    for keyword in language_keywords:\n",
    "        language_start = text.find(keyword)\n",
    "        if language_start != -1:\n",
    "            lines = text.splitlines()[language_start:]\n",
    "            languages = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                language_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if language_match:\n",
    "                    languages.append(language_match.group())\n",
    "            return languages\n",
    "    return []\n",
    "\n",
    "# Function to extract certificates\n",
    "def extract_certificates(text):\n",
    "    certificate_keywords = [\"certifications\", \"accreditations\"]\n",
    "    for keyword in certificate_keywords:\n",
    "        certificate_start = text.find(keyword)\n",
    "        if certificate_start != -1:\n",
    "            lines = text.splitlines()[certificate_start:]\n",
    "            certificates = []\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                certificate_match = re.search(r\"[A-Z][a-z]+(?:\\s[A-Z][a-z]+)?\", line, re.IGNORECASE)\n",
    "                if certificate_match:\n",
    "                    certificates.append(certificate_match.group())\n",
    "            return certificates\n",
    "    return []\n",
    "def extract_skills(text):\n",
    "    # Predefined list of common technical and soft skills\n",
    "    skills_list = [\n",
    "        \"C\", \"C++\", \"Python\", \"Java\", \"SQL\", \"JavaScript\", \"HTML\", \"CSS\", \"React\", \"Angular\",\n",
    "        \"Machine Learning\", \"Deep Learning\", \"Data Structures\", \"Data Analysis\", \"Project Management\",\n",
    "        \"Leadership\", \"Communication\", \"Teamwork\", \"Problem Solving\", \"Critical Thinking\",\n",
    "        \"Agile\", \"Scrum\", \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"DevOps\", \"Data Science\",\n",
    "        \"R\", \"Excel\", \"Power BI\", \"Tableau\", \"Automation\", \"Selenium\", \"REST APIs\", \n",
    "        \"Networking\", \"Cybersecurity\", \"Cloud Computing\", \"Blockchain\", \"UI/UX Design\",\n",
    "        \"Customer Service\", \"Salesforce\", \"SAP\", \"Digital Marketing\", \"SEO\", \"Content Writing\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the skills list to lowercase for case-insensitive matching\n",
    "    skills_set = set(skill.lower() for skill in skills_list)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Extract the skills by checking if any word in the text matches the skills list\n",
    "    extracted_skills = [word for word in words if word.lower() in skills_set]\n",
    "    \n",
    "    # Extract skill lines that might include proficiency symbols (e.g., \"○ ○ ○ ○ ○\")\n",
    "    proficiency_pattern = r\"([A-Za-z\\s,]+)\\s*([○●]{1,5})\"\n",
    "    proficiency_matches = re.findall(proficiency_pattern, text)\n",
    "    \n",
    "    # Add skills from proficiency matches\n",
    "    for match in proficiency_matches:\n",
    "        skill = match[0].strip()\n",
    "        if skill.lower() in skills_set:\n",
    "            extracted_skills.append(skill)\n",
    "\n",
    "    # Extract skills listed in a comma-separated format\n",
    "    skills_pattern = r\"([A-Za-z\\s\\+\\#\\&]+(?:,\\s*[A-Za-z\\s\\+\\#\\&]+)*)\"\n",
    "    skills_matches = re.findall(skills_pattern, text)\n",
    "    for match in skills_matches:\n",
    "        for skill in match.split(','):\n",
    "            cleaned_skill = skill.strip()\n",
    "            if cleaned_skill.lower() in skills_set:\n",
    "                extracted_skills.append(cleaned_skill)\n",
    "\n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    unique_skills = list(set(extracted_skills))\n",
    "    \n",
    "    return unique_skills if unique_skills else [\"Not Found\"]\n",
    "\n",
    "\n",
    "# Function to process all PDFs in a folder and save the extracted info in a CSV\n",
    "def process_resumes(folder_path, output_csv_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            name = extract_name(text)\n",
    "            profile_summary = extract_profile_summary(text)\n",
    "            email = extract_email(text)\n",
    "            phone = extract_phone_number(text)\n",
    "            address = extract_address(text)\n",
    "            links = extract_links(text)\n",
    "            experience = extract_experience(text)\n",
    "            education = extract_education(text)\n",
    "            languages = extract_languages(text)\n",
    "            certificates = extract_certificates(text)\n",
    "            skills = extract_skills(text)\n",
    "\n",
    "            data.append({\n",
    "                'File Name': filename,\n",
    "                'Name': name,\n",
    "                'Profile Summary': profile_summary,\n",
    "                'Email': email,\n",
    "                'Phone': phone,\n",
    "                'Address': address,\n",
    "                'Links': links,\n",
    "                'Experience': experience,\n",
    "                'Education': education,\n",
    "                'Languages': languages,\n",
    "                'Certificates': certificates,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if os.path.exists(output_csv_path):\n",
    "        os.remove(output_csv_path)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Data saved to {output_csv_path}')\n",
    "\n",
    "process_resumes('C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\\\data\\\\BPO\\\\bb', 'chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e89f516-e2ed-42cf-8330-cd0bc5dbf437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills from C:/Users/santhoshs.s/jupyter/resumes/data/data/BPO/bb/Amsterdam-Modern-Resume-Template.pdf:\n",
      "['c', 'r']\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract skills from the text\n",
    "def extract_skills(text):\n",
    "    # Predefined list of common technical and soft skills\n",
    "    skills_list = [\n",
    "        \"C\", \"C++\", \"Python\", \"Java\", \"SQL\", \"JavaScript\", \"HTML\", \"CSS\", \"React\", \"Angular\",\n",
    "        \"Machine Learning\", \"Deep Learning\", \"Data Structures\", \"Data Analysis\", \"Project Management\",\n",
    "        \"Leadership\", \"Communication\", \"Teamwork\", \"Problem Solving\", \"Critical Thinking\",\n",
    "        \"Agile\", \"Scrum\", \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"DevOps\", \"Data Science\",\n",
    "        \"R\", \"Excel\", \"Power BI\", \"Tableau\", \"Automation\", \"Selenium\", \"REST APIs\", \n",
    "        \"Networking\", \"Cybersecurity\", \"Cloud Computing\", \"Blockchain\", \"UI/UX Design\",\n",
    "        \"Customer Service\", \"Salesforce\", \"SAP\", \"Digital Marketing\", \"SEO\", \"Content Writing\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the skills list to lowercase for case-insensitive matching\n",
    "    skills_set = set(skill.lower() for skill in skills_list)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Extract the skills by checking if any word in the text matches the skills list\n",
    "    extracted_skills = [word for word in words if word.lower() in skills_set]\n",
    "    \n",
    "    # Extract skill lines that might include proficiency symbols (e.g., \"○ ○ ○ ○ ○\")\n",
    "    proficiency_pattern = r\"([A-Za-z\\s,]+)\\s*([○●]{1,5})\"\n",
    "    proficiency_matches = re.findall(proficiency_pattern, text)\n",
    "    \n",
    "    # Add skills from proficiency matches\n",
    "    for match in proficiency_matches:\n",
    "        skill = match[0].strip()\n",
    "        if skill.lower() in skills_set:\n",
    "            extracted_skills.append(skill)\n",
    "\n",
    "    # Extract skills listed in a comma-separated format\n",
    "    skills_pattern = r\"([A-Za-z\\s\\+\\#\\&]+(?:,\\s*[A-Za-z\\s\\+\\#\\&]+)*)\"\n",
    "    skills_matches = re.findall(skills_pattern, text)\n",
    "    for match in skills_matches:\n",
    "        for skill in match.split(','):\n",
    "            cleaned_skill = skill.strip()\n",
    "            if cleaned_skill.lower() in skills_set:\n",
    "                extracted_skills.append(cleaned_skill)\n",
    "\n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    unique_skills = list(set(extracted_skills))\n",
    "    \n",
    "    return unique_skills if unique_skills else [\"Not Found\"]\n",
    "\n",
    "# Function to parse a PDF file and display extracted skills\n",
    "def parse_pdf_and_extract_skills(pdf_path):\n",
    "    # Extract text from the given PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Extract skills from the text\n",
    "    skills = extract_skills(text)\n",
    "    \n",
    "    # Display the extracted skills\n",
    "    print(f\"Extracted Skills from {pdf_path}:\")\n",
    "    print(skills)\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'sample_resume.pdf' with the path to your PDF file\n",
    "pdf_path = 'C:/Users/santhoshs.s/jupyter/resumes/data/data/BPO/bb/Amsterdam-Modern-Resume-Template.pdf'\n",
    "parse_pdf_and_extract_skills(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb481a7-00fd-427b-abcd-ec022ea01fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'julie monroe\\nnutrition consultant\\nd e t a i l s\\naddress\\n1515 pacific ave\\nlos angeles, ca 90291\\nunited states\\nphone\\n3868683442\\nemail\\nemail@email.com\\nplace of birth\\nsan antonio\\ndriving license\\nfull\\nl i n k s\\nlinkedin\\npinterest\\nresume templates\\nbuild this template\\ns k i l l s\\nfood preparation\\nkitchen maintenance\\nkitchen equipment \\noperation\\nfood sanitation\\nnutrition\\nh o b b i e s\\nsoccer, rugby, tennis\\nl a n g u a g e s\\nenglish\\np r o f i l e\\ntalented nutrition consultant with three years of experience. skilled in nutrition \\nand food preparation and looking to deliver healthy, delicious meals at woodacre \\nnursing home. at 7-star senior living, cheerfully cleaned kitchens and prepared \\nthree meals daily for 120+ residents. received a promotion to head nutrition \\nconsultant within five months of hiring due to efficiency and interpersonal skills.\\ne m p l o y m e n t  h i s t o r y\\nnutritional consultant (part-time) , wic\\nport washington\\njan 2021 — present\\nrequired to prescribe supplemental foo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Path to the provided PDF file\n",
    "pdf_path = 'C:/Users/santhoshs.s/jupyter/resumes/data/data/BPO/bb/Amsterdam-Modern-Resume-Template.pdf'\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display the extracted text\n",
    "extracted_text[:1000]  # Displaying only the first 1000 characters for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc9913-6a88-48c5-bb3d-e8cca55039da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe334b-04c9-410f-bdeb-d1562016361f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc94cb-8601-4af4-9eb5-b1a66b79fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565909b1-a4bc-42d9-b4e1-d0101cb930d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7597e-577d-47a2-9fa6-dab552074813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466551c-804a-4cce-9161-34e7933a23f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84066f7c-d60b-499b-82e4-e2762634f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ea8f7-1fb3-47b2-a1f8-956680f0f4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac707e-bb2d-4553-be69-66fdbba76eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926e5dc-b77e-44ba-b4ad-26188fb3933d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106cf57-013c-4c6c-835c-dff017ccefda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f708d35-ae19-4964-8a7b-eb1aa333092b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134bca6-1f7f-47e6-b5f4-309178983d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed18491-d9e0-4adc-b9c9-471ac6d17536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:197: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:197: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\santhoshs.s\\AppData\\Local\\Temp\\ipykernel_15564\\124601064.py:197: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  resume_path = \"C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\data\\\\BPO\\\\bb\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\santhoshs.s\\jupyter\\resumes\\data\\data\\BPO\\bb: 'C:\\Users\\santhoshs.s\\jupyter\\resumes\\data\\data\\BPO\\bb' is no file\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Predefined lists and compiled regex patterns\n",
    "name_patterns = [\n",
    "    re.compile(r\"[A-Z][a-z]+ [A-Z][a-z]+\"),  # First + Last name\n",
    "    re.compile(r\"[A-Z][a-z]+-\\w+\")  # Hyphenated names\n",
    "]\n",
    "\n",
    "email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "phone_pattern = re.compile(r'\\b(?:\\+?(\\d{1,3}))?[-. ()]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b')\n",
    "\n",
    "address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "\n",
    "keywords = [\n",
    "    \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "    \"career summary\", \"executive summary\", \"personal summary\",\n",
    "    \"summary of qualifications\", \"overview\", \"Profile\"\n",
    "]\n",
    "\n",
    "experience_keywords = [\n",
    "    \"work history\", \"employment history\", \"work experience\",\n",
    "    \"professional experience\", \"career summary\", \"professional background\",\n",
    "    \"job experience\", \"internships\", \"relevant experience\",\n",
    "    \"contract work\", \"military experience\", \"volunteer work\"\n",
    "]\n",
    "\n",
    "date_patterns = [\n",
    "    re.compile(r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\"),\n",
    "    re.compile(r\"\\b\\d{4}\\s*[-–—to]+\\s*\\d{4}\\b\"),\n",
    "    re.compile(r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-–—to]+\\s*Present\"),\n",
    "    re.compile(r\"\\b\\d{4}\\s*[-–—to]+\\s*Present\\b\")\n",
    "]\n",
    "\n",
    "# Function to extract text from PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        return \"\".join(page.get_text() for page in doc).lower()\n",
    "\n",
    "# Function to extract candidate's name\n",
    "def extract_name(text):\n",
    "    for pattern in name_patterns:\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract profile summary\n",
    "def extract_profile_summary(text):\n",
    "    lines = text.splitlines()\n",
    "    for idx, line in enumerate(lines):\n",
    "        if any(keyword.lower() in line.lower() for keyword in keywords):\n",
    "            summary_start = idx + 1\n",
    "            break\n",
    "    else:\n",
    "        return \"Not Found\"\n",
    "\n",
    "    summary_lines = []\n",
    "    for line in lines[summary_start:]:\n",
    "        line = line.strip()\n",
    "        if line == \"\" or line.startswith((\"*\", \"#\")) or line.isupper():\n",
    "            break\n",
    "        summary_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(summary_lines).strip() or \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    match = email_pattern.search(text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract phone number\n",
    "def extract_phone_number(text):\n",
    "    match = phone_pattern.search(text)\n",
    "    return re.sub(r'\\D', '', match.group(0)) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract address\n",
    "def extract_address(text):\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            return line.strip()\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract links\n",
    "def extract_links(text):\n",
    "    link_patterns = {\n",
    "        'LinkedIn': r'https?://www\\.linkedin\\.com/in/[a-zA-Z0-9-]*',\n",
    "        'GitHub': r'https?://github\\.com/[a-zA-Z0-9-]*',\n",
    "        'Kaggle': r'https?://www\\.kaggle\\.com/([a-zA-Z0-9-]*|profile/[a-zA-Z0-9-]*)',\n",
    "        'Portfolio': r'https?://(www\\.)?[a-zA-Z0-9-]*\\.(com|net|org|io|in)/?',\n",
    "        'Facebook': r'https?://www\\.facebook\\.com/([a-zA-Z0-9-]*|profile\\.php\\?id=\\d+)',\n",
    "        'Instagram': r'https?://www\\.instagram\\.com/([a-zA-Z0-9_]*)',\n",
    "        'Git': r'https?://(git|gitlab|bitbucket)\\.(com|org)/[a-zA-Z0-9-]*/[a-zA-Z0-9-]*',\n",
    "        'Monster': r'https?://www\\.monster\\.com/jobs/search/?',\n",
    "        'Email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "        'Standard URLs': r'https?://[^\\s]+',\n",
    "        'HTML Links': r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']',\n",
    "        'Markdown Links': r'\\[.*?\\]\\((https?://[^\\s]+)\\)',\n",
    "        'Plain Links': r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "    }\n",
    "\n",
    "    links_dict = {key: re.findall(pattern, text) for key, pattern in link_patterns.items()}\n",
    "\n",
    "    # Combine all found links into a single list and remove duplicates\n",
    "    all_links = set()\n",
    "    for links in links_dict.values():\n",
    "        all_links.update(links)\n",
    "\n",
    "    links_dict['Other'] = list(all_links - set(links_dict['LinkedIn']) - set(links_dict['GitHub']) - set(links_dict['Email']))\n",
    "    \n",
    "    return links_dict\n",
    "\n",
    "# Function to extract experience details\n",
    "def extract_experience(text):\n",
    "    if not re.search('|'.join(experience_keywords), text, re.IGNORECASE):\n",
    "        return []\n",
    "\n",
    "    content = text\n",
    "    dates = [date_pattern.findall(content) for date_pattern in date_patterns]\n",
    "    dates = [date for sublist in dates for date in sublist]  # Flatten the list\n",
    "\n",
    "    job_entries = re.split('|'.join(date_patterns), content)[1:]  # Split content and skip the first part\n",
    "    experiences = []\n",
    "\n",
    "    for date, job in zip(dates, job_entries):\n",
    "        job_title = next((title for title in re.findall(r'\\b\\w+\\b', job) if title.lower() in job_title_keywords), '')\n",
    "        company_match = re.search(r\"(?:at|with|for)\\s+([A-Z][\\w&,. ]+)\", job)\n",
    "        location_match = re.search(r\"\\bin\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b\", job)\n",
    "\n",
    "        experiences.append({\n",
    "            \"Company\": company_match.group(1) if company_match else '',\n",
    "            \"Job Title\": job_title,\n",
    "            \"Location\": location_match.group(1) if location_match else '',\n",
    "            \"Dates\": date,\n",
    "            \"Responsibilities\": ' '.join(re.findall(r\"•\\s*.*|(?<=\\n)-\\s*.*\", job)).strip()\n",
    "        })\n",
    "\n",
    "    return experiences\n",
    "\n",
    "# Function to extract education details\n",
    "def extract_education(text):\n",
    "    education_list = []\n",
    "    \n",
    "    degree_patterns = re.compile(r\"(Bachelor|Master|Associate|Doctorate|Ph\\.?D|Diploma|Certificate)\\s*(of\\s*(Arts|Science|Business|Engineering|Technology|Management|Education|Commerce|Law|Medicine|Fine Arts|Social Work|Computer Science|Psychology|Nursing|Physics|Chemistry|Biology))?|(?:BA|BS|BSc|MA|MS|MBA|MFA|LLB|LLM|PhD|M\\.Ed|B\\.Ed|BBA)\")\n",
    "    university_patterns = re.compile(r\"[A-Z][a-zA-Z\\s&]*\\s*(University|Institute|College|Academy|Polytechnic)\")\n",
    "    time_period_pattern = re.compile(r\"\\b\\d{4}\\s*[-–—to]+\\s*(?:Present|\\d{4})\\b\")\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if degree_patterns.search(line):\n",
    "            degree = degree_patterns.search(line).group()\n",
    "            university = university_patterns.search(line)\n",
    "            time_period = time_period_pattern.search(line)\n",
    "\n",
    "            education_list.append({\n",
    "                'Degree': degree,\n",
    "                'University': university.group() if university else 'Not Found',\n",
    "                'Time Period': time_period.group() if time_period else 'Not Found'\n",
    "            })\n",
    "\n",
    "    return education_list\n",
    "\n",
    "# Main function to extract all details from the resume\n",
    "def extract_resume_details(resume_path):\n",
    "    try:\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        return {\n",
    "            \"Name\": extract_name(text),\n",
    "            \"Profile Summary\": extract_profile_summary(text),\n",
    "            \"Email\": extract_email(text),\n",
    "            \"Phone\": extract_phone_number(text),\n",
    "            \"Address\": extract_address(text),\n",
    "            \"Links\": extract_links(text),\n",
    "            \"Experience\": extract_experience(text),\n",
    "            \"Education\": extract_education(text)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {resume_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    resume_path = \"C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\resumes\\\\data\\data\\\\BPO\\\\bb\"\n",
    "    details = extract_resume_details(resume_path)\n",
    "    print(details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a2c98-5ae1-4680-8921-5f233c281c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
