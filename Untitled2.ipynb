{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23c780-9916-401a-beb9-6ff714c33293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "def extract_profile_summary(text):\n",
    "    # List of potential keywords indicating the profile section\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "\n",
    "    # Normalize keywords: Create regex patterns to ignore spaces\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    # Attempt to find the starting point of the summary based on keywords\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):  # Check for keyword with regex\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    # If a summary section was found, extract subsequent lines\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        \n",
    "        # Start extracting from the next line after the keyword\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # Stop extraction if the line is empty or contains bullet points, but allow for continuation\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue  # Allow extraction to continue past bullet points and empty lines\n",
    "            if line.isupper():  # Stop if we reach a fully uppercase line, which may indicate a section title\n",
    "                break\n",
    "            \n",
    "            # Add the line to the extracted lines\n",
    "            extracted_lines.append(line)\n",
    "\n",
    "        # Join the extracted lines into a single summary string\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return match.group(0).replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\" \", \"\") if match else \"Not Found\"\n",
    "\n",
    "# Function to extract address without including the keyword \"address\"\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Iterate through the lines to find keywords\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Check if any of the address keywords are in the line\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            # Extract lines after the keyword to capture the full address\n",
    "            extracted_lines = []\n",
    "            \n",
    "            # If the keyword \"address\" is present in the line, ignore it and start from the next part of the line\n",
    "            clean_line = re.sub(r'\\b(?:' + '|'.join(address_keywords) + r')\\b', '', line, flags=re.IGNORECASE).strip()\n",
    "            if clean_line:\n",
    "                extracted_lines.append(clean_line)\n",
    "            \n",
    "            # Attempt to include a few subsequent lines in case the address continues\n",
    "            for i in range(1, 3):\n",
    "                if idx + i < len(lines):\n",
    "                    next_line = lines[idx + i].strip()\n",
    "                    if next_line:\n",
    "                        extracted_lines.append(next_line)\n",
    "            \n",
    "            # Combine the extracted lines into a single string\n",
    "            return \" \".join(extracted_lines).strip()\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract links\n",
    "def extract_links(text):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from text, including standard URLs, HTML-like links, and markdown links.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text from which to extract links.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted hyperlinks or \"Not Found\" if no links are found.\n",
    "    \"\"\"\n",
    "    # Standard URLs\n",
    "    link_pattern = r'https?://[^\\s]+'\n",
    "    \n",
    "    # HTML links\n",
    "    html_link_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']'\n",
    "    \n",
    "    # Markdown links\n",
    "    markdown_link_pattern = r'\\[.*?\\]\\((https?://[^\\s]+)\\)'\n",
    "    \n",
    "    # Plain links like www.example.com or example.com\n",
    "    plain_link_pattern = r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "    \n",
    "    # Find all matches for each pattern\n",
    "    standard_links = re.findall(link_pattern, text)\n",
    "    html_links = re.findall(html_link_pattern, text)\n",
    "    markdown_links = re.findall(markdown_link_pattern, text)\n",
    "    plain_links = re.findall(plain_link_pattern, text)\n",
    "    \n",
    "    # Combine all found links into a single list\n",
    "    hyperlinks = standard_links + html_links + markdown_links + plain_links\n",
    "    \n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_links = list(set(hyperlinks))\n",
    "    \n",
    "    return unique_links if unique_links else [\"Not Found\"]\n",
    "\n",
    "# Function to extract experience details\n",
    "def extract_experience(text):\n",
    "    # Define keywords for detecting experience section headers\n",
    "    experience_section_keywords = [\n",
    "        \"work history\", \"employment history\", \"work experience\",\n",
    "        \"professional experience\", \"career summary\", \"professional background\",\n",
    "        \"job experience\", \"internships\", \"relevant experience\",\n",
    "        \"contract work\", \"military experience\", \"volunteer work\"\n",
    "    ]\n",
    "    \n",
    "    # Define date patterns for recognizing job durations\n",
    "    date_patterns = [\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-‚Äì‚Äîto]+\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\",\n",
    "        r\"\\b\\d{4}\\s*[-‚Äì‚Äîto]+\\s*\\d{4}\\b\",\n",
    "        r\"\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\\s*[-‚Äì‚Äîto]+\\s*Present\",\n",
    "        r\"\\b\\d{4}\\s*[-‚Äì‚Äîto]+\\s*Present\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Define keywords for recognizing job titles\n",
    "    job_title_keywords = [\n",
    "        \"manager\", \"director\", \"engineer\", \"analyst\", \"consultant\",\n",
    "        \"assistant\", \"coordinator\", \"specialist\", \"developer\", \"designer\",\n",
    "        \"executive\", \"advisor\", \"technician\", \"officer\", \"intern\", \"trainee\"\n",
    "    ]\n",
    "    \n",
    "    # Patterns for extracting company names\n",
    "    company_patterns = r\"(at|with|for)\\s+([A-Z][\\w&,. ]+)\"\n",
    "    \n",
    "    # Patterns for extracting location information\n",
    "    location_patterns = [\n",
    "        r\"\\bin\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b\",\n",
    "        r\"\\b([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\s+(USA|Inc|Ltd|LLC|Co)\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Patterns for extracting responsibilities\n",
    "    responsibility_patterns = [\n",
    "        r\"(managed|developed|designed|led|coordinated|executed|oversaw|achieved|conducted|implemented|increased|improved|created|initiated)\\s+[^\\n]*\",\n",
    "        r\"(responsible for|tasked with|in charge of|oversaw|managed|coordinated|led)\\s+[^\\n]*\"\n",
    "    ]\n",
    "    \n",
    "    experience_sections = []\n",
    "    \n",
    "    # Split text into lines for processing\n",
    "    lines = text.splitlines()\n",
    "    experience_section_found = False\n",
    "\n",
    "    # Iterate through lines to find experience sections\n",
    "    for line in lines:\n",
    "        # Check for experience section headers\n",
    "        if any(keyword in line.lower() for keyword in experience_section_keywords):\n",
    "            experience_section_found = True\n",
    "            continue\n",
    "        \n",
    "        # If we found the experience section, start extracting\n",
    "        if experience_section_found:\n",
    "            if line.strip() == \"\" or line.isupper():  # Stop extraction on empty lines or titles\n",
    "                break\n",
    "            \n",
    "            # Extract job title\n",
    "            job_title = None\n",
    "            for keyword in job_title_keywords:\n",
    "                if keyword in line.lower():\n",
    "                    job_title = line.strip()\n",
    "                    break\n",
    "\n",
    "            # Extract company name\n",
    "            company_match = re.search(company_patterns, line)\n",
    "            company_name = company_match.group(2).strip() if company_match else \"Not Found\"\n",
    "\n",
    "            # Extract location\n",
    "            location = \"Not Found\"\n",
    "            for pattern in location_patterns:\n",
    "                location_match = re.search(pattern, line)\n",
    "                if location_match:\n",
    "                    location = location_match.group(1).strip()\n",
    "                    break\n",
    "\n",
    "            # Extract dates\n",
    "            dates = \"Not Found\"\n",
    "            for pattern in date_patterns:\n",
    "                date_match = re.search(pattern, line)\n",
    "                if date_match:\n",
    "                    dates = date_match.group().strip()\n",
    "                    break\n",
    "            \n",
    "            # Extract responsibilities\n",
    "            responsibilities = []\n",
    "            for pattern in responsibility_patterns:\n",
    "                responsibility_matches = re.findall(pattern, line)\n",
    "                responsibilities.extend(responsibility_matches)\n",
    "\n",
    "            # Create a structured experience dictionary\n",
    "            experience_entry = {\n",
    "                \"job_title\": job_title if job_title else \"Not Found\",\n",
    "                \"company_name\": company_name,\n",
    "                \"location\": location,\n",
    "                \"dates\": dates,\n",
    "                \"responsibilities\": \" | \".join(responsibilities) if responsibilities else \"Not Found\"\n",
    "            }\n",
    "\n",
    "            # Add entry to experience sections\n",
    "            experience_sections.append(experience_entry)\n",
    "    \n",
    "    return experience_sections if experience_sections else \"Not Found\"\n",
    "\n",
    "# Function to extract all relevant data from the CV\n",
    "def extract_data_from_cv(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    name = extract_name(text)\n",
    "    profile_summary = extract_profile_summary(text)\n",
    "    email = extract_email(text)\n",
    "    phone_number = extract_phone_number(text)\n",
    "    address = extract_address(text)\n",
    "    links = extract_links(text)\n",
    "    experience = extract_experience(text)  # Replaced function name\n",
    "\n",
    "    # Create a dictionary to store the extracted data\n",
    "    extracted_data = {\n",
    "        \"name\": name,\n",
    "        \"profile_summary\": profile_summary,\n",
    "        \"email\": email,\n",
    "        \"phone_number\": phone_number,\n",
    "        \"address\": address,\n",
    "        \"links\": links,\n",
    "        \"experience\": experience\n",
    "    }\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Main block to run the extraction process\n",
    "if __name__ == \"__main__\":\n",
    "    # Example PDF path (make sure to replace it with the actual path)\n",
    "    pdf_file_path = 'path/to/cv.pdf'  # Adjust this path accordingly\n",
    "    if os.path.exists(pdf_file_path):\n",
    "        extracted_info = extract_data_from_cv(pdf_file_path)\n",
    "        print(pd.DataFrame([extracted_info]))  # Print as DataFrame for better visualization\n",
    "    else:\n",
    "        print(\"File not found. Please check the path and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b24365-18e0-410d-85d7-58bc52f42433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      " anuva goyal\n",
      "[ anuvagoyal111@gmail.com\n",
      "¬Ω agra, uttar pradesh, india\n",
      "¬á github.com/anuvagoyal\n",
      "projects\n",
      "‚Ä¢ mental healthcare chatbot that provides ad-\n",
      "vice to the user based on diÔ¨Äerent categories\n",
      "of mental health problems using a dataset\n",
      "webscraped from counselchat.com (nov 2021)\n",
      "‚Ä¢ full stack speech emotion based movie\n",
      "recommender system using the ravdess\n",
      "dataset and web scraping techniques (oct\n",
      "2021)\n",
      "‚Ä¢ finding a perfect fit, a model to parse re-\n",
      "sumes using pytesseract, nlp and xg boost\n",
      "and random forest classiÔ¨Åcation techniques\n",
      "(aug 2021)\n",
      "‚Ä¢ face mask detection which detects the face\n",
      "using haar cascade classiÔ¨Åer and classiÔ¨Åes\n",
      "the image into one of the three categories-\n",
      "without mask, with mask and incorrect mask\n",
      "(may 2021)\n",
      "education\n",
      "b.tech, electrical engineering with\n",
      "computer science specialisation\n",
      "dayalbagh educational institute, agra\n",
      "\u0011 july 2019‚Äì present (sgpa 9.35 - 4 sem)\n",
      "higher secondary school certiÔ¨Åcate\n",
      "st. clare‚Äôs senior secondary school, agra\n",
      "\u0011 2019 (94%)\n",
      "secondary school certiÔ¨Åcate\n",
      "st. clare‚Äôs senior secondary school, agra\n",
      "\u0011 2017 (cgpa 10)\n",
      "publication\n",
      "researchgate.net/proÔ¨Åle/anuva_goyal\n",
      "skills\n",
      "c, c++\n",
      "‚óã‚óã‚óã‚óã‚óã\n",
      "python, sql\n",
      "‚óã‚óã‚óã‚óã‚óã\n",
      "data structures\n",
      "‚óã‚óã‚óã‚óã‚óã\n",
      "css, html\n",
      "‚óã‚óã‚óã‚óã‚óã\n",
      "machine learning frameworks: numpy,\n",
      "pandas, tensorÔ¨Çow, scikit-learn, opencv,\n",
      "pytesseract, beautifulsoup\n",
      "interests\n",
      "‚Ä¢ ml and deep learning.\n",
      "‚Ä¢ nlp and computer vision\n",
      "experience\n",
      "summer intern\n",
      "genisup india pvt. ltd., hosur, tamil nadu\n",
      "\u0011 june 2021 ‚Äì aug 2021\n",
      "¬Ω remote\n",
      "‚Ä¢ internship on the topic nlp: topic modeling to assign the\n",
      "theme or topic for any news article on internet using machine\n",
      "learning techniques.\n",
      "‚Ä¢ worked on proxy rotation and web scraping\n",
      "‚Ä¢ performed lda topic modeling on ‚Äúthe hindu‚Äù news articles\n",
      "and obtained precision score of 0.906.\n",
      "intern trainee\n",
      "vugs technologies pvt. ltd., agra, uttar pradesh\n",
      "\u0011 may 2021 ‚Äì june 2021\n",
      "¬Ω remote\n",
      "‚Ä¢ built an ocr using pytesseract and ner text classiÔ¨Åcation\n",
      "model to categorize detected text into name, e-mail address,\n",
      "phone number and date using nltk,spacy and bert\n",
      "‚Ä¢ created an ocr for handwritten text [a-z, 0-9] using cnn\n",
      "architecture\n",
      "‚Ä¢ built a face recognition model and face mask detection\n",
      "model using opencv and haar cascade classiÔ¨Åer.\n",
      "trainings\n",
      "deep learning and computer vision a-z: opencv,\n",
      "ssd, gans\n",
      "udemy\n",
      "\u0011 nov 2021 ‚Äì dec 2021\n",
      "30 days of google cloud\n",
      "google llc\n",
      "\u0011 oct 2021 ‚Äì nov 2021\n",
      "neural networks and deep learning\n",
      "coursera\n",
      "\u0011 mar 2021 ‚Äì apr 2021\n",
      "machine learning a-z: hands on python and r\n",
      "in data science\n",
      "udemy\n",
      "\u0011 dec 2020 ‚Äì feb 2021\n",
      "achievements\n",
      "‚Ä¢ speaker at ml/ai event organized by the google developers\n",
      "student club, dei (dec 2021)\n",
      "‚Ä¢ secured 3rd position in tech-a-thon organized by the ece\n",
      "society, bit mesra, ranchi (oct 2021)\n",
      "‚Ä¢ attended online kla workshop on ai and hpc in semicon-\n",
      "ductor manufacturing organized by iit madras (sep 2021)\n",
      "‚Ä¢ won 1st prize in the online competition game of brands orga-\n",
      "nized by sggscc, university of delhi (mar 2021)\n",
      "\n",
      "No experiences found.\n",
      "Data extracted and saved to output7.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract text from PDF file and convert it to lowercase\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.lower()  # Convert the entire text to lowercase\n",
    "\n",
    "# Function to extract the candidate's name\n",
    "def extract_name(text):\n",
    "    name_patterns = [\n",
    "        r\"[A-Z][a-z]+ [A-Z][a-z]+\",  # Two words with capital letters (first name + last name)\n",
    "        r\"[A-Z][a-z]+-\\w+\"  # Hyphenated names like \"Anne-Marie\"\n",
    "    ]\n",
    "\n",
    "    for pattern in name_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive matching\n",
    "        if match:\n",
    "            return match.group()\n",
    "\n",
    "    # If regular expressions fail, try using NLTK for Named Entity Recognition (NER)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "        for word, tag in tagged_text:\n",
    "            if tag == 'NNP':  # Proper noun, typically used for names\n",
    "                return word\n",
    "    except LookupError:\n",
    "        print(\"NLTK data not downloaded. Install NLTK and run 'nltk.download()' to download required resources.\")\n",
    "\n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract the profile summary\n",
    "def extract_profile_summary(text):\n",
    "    # List of potential keywords indicating the profile section\n",
    "    keywords = [\n",
    "        \"summary\", \"profile\", \"objective\", \"professional summary\",\n",
    "        \"career summary\", \"executive summary\", \"personal summary\",\n",
    "        \"summary of qualifications\", \"overview\"\n",
    "    ]\n",
    "    \n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    lines = text.splitlines()  # Split the text into lines\n",
    "    summary_start = None\n",
    "\n",
    "    # Normalize keywords: Create regex patterns to ignore spaces\n",
    "    keyword_patterns = [re.compile(r'\\s*'.join(list(keyword.lower()))) for keyword in keywords]\n",
    "\n",
    "    # Attempt to find the starting point of the summary based on keywords\n",
    "    for pattern in keyword_patterns:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if pattern.search(line.lower()):  # Check for keyword with regex\n",
    "                summary_start = idx\n",
    "                break\n",
    "        if summary_start is not None:\n",
    "            break\n",
    "\n",
    "    # If a summary section was found, extract subsequent lines\n",
    "    if summary_start is not None:\n",
    "        extracted_lines = []\n",
    "        \n",
    "        # Start extracting from the next line after the keyword\n",
    "        for i in range(summary_start + 1, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # Stop extraction if the line is empty or contains bullet points, but allow for continuation\n",
    "            if line == \"\" or line.startswith(\"*\") or line.startswith(\"#\"):\n",
    "                continue  # Allow extraction to continue past bullet points and empty lines\n",
    "            if line.isupper():  # Stop if we reach a fully uppercase line, which may indicate a section title\n",
    "                break\n",
    "            \n",
    "            # Add the line to the extracted lines\n",
    "            extracted_lines.append(line)\n",
    "\n",
    "        # Join the extracted lines into a single summary string\n",
    "        summary_result = \"\\n\".join(extracted_lines).strip()\n",
    "        return summary_result if summary_result else \"Not Found\"\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract email address\n",
    "def extract_email(text):\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Not Found\"\n",
    "\n",
    "# Function to extract the phone number\n",
    "def extract_phone_number(text):\n",
    "    phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return match.group(0).replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\" \", \"\") if match else \"Not Found\"\n",
    "\n",
    "def extract_address(text):\n",
    "    address_keywords = ['address', 'street', 'city', 'state', 'zip', 'postal']\n",
    "    location_symbols = ['üìç', 'üìå', 'üìç', 'üî∂','üè†','¬Ω']  # Add more symbols as needed\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Iterate through the lines to find keywords\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Check if any of the address keywords are in the line\n",
    "        if any(keyword in line.lower() for keyword in address_keywords):\n",
    "            # Extract lines after the keyword to capture the full address\n",
    "            extracted_lines = []\n",
    "            \n",
    "            # If the keyword \"address\" is present in the line, ignore it and start from the next part of the line\n",
    "            clean_line = re.sub(r'\\b(?:' + '|'.join(address_keywords) + r')\\b', '', line, flags=re.IGNORECASE).strip()\n",
    "            if clean_line:\n",
    "                extracted_lines.append(clean_line)\n",
    "            \n",
    "            # Attempt to include a few subsequent lines in case the address continues\n",
    "            for i in range(1, 3):\n",
    "                if idx + i < len(lines):\n",
    "                    next_line = lines[idx + i].strip()\n",
    "                    if next_line:\n",
    "                        extracted_lines.append(next_line)\n",
    "\n",
    "            # Check for symbols in the extracted lines\n",
    "            for symbol in location_symbols:\n",
    "                if symbol in ' '.join(extracted_lines):\n",
    "                    return \" \".join(extracted_lines).strip() + \" \" + symbol\n",
    "            \n",
    "            # Combine the extracted lines into a single string\n",
    "            return \" \".join(extracted_lines).strip()\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# Function to extract hyperlinks\n",
    "def extract_links(text):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from text, including standard URLs, HTML-like links, and markdown links.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text from which to extract links.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted hyperlinks or \"Not Found\" if no links are found.\n",
    "    \"\"\"\n",
    "    # Standard URLs\n",
    "    link_pattern = r'https?://[^\\s]+'\n",
    "    \n",
    "    # HTML links\n",
    "    html_link_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](https?://[^\\s\"\\']+)[\"\\']'\n",
    "    \n",
    "    # Markdown links\n",
    "    markdown_link_pattern = r'\\[.*?\\]\\((https?://[^\\s]+)\\)'\n",
    "    \n",
    "    # Plain links like www.example.com or example.com\n",
    "    plain_link_pattern = r'\\b(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?\\b'\n",
    "    \n",
    "    # Find all matches for each pattern\n",
    "    standard_links = re.findall(link_pattern, text)\n",
    "    html_links = re.findall(html_link_pattern, text)\n",
    "    markdown_links = re.findall(markdown_link_pattern, text)\n",
    "    plain_links = re.findall(plain_link_pattern, text)\n",
    "    \n",
    "    # Combine all found links into a single list\n",
    "    hyperlinks = standard_links + html_links + markdown_links + plain_links\n",
    "    \n",
    "    # Remove duplicates by converting to a set, then back to a list\n",
    "    unique_links = list(set(hyperlinks))\n",
    "    \n",
    "    return unique_links if unique_links else [\"Not Found\"]\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_experience(text):\n",
    "    # Define keywords for detecting experience section headers\n",
    "    experience_section_keywords = [\n",
    "        \"work history\", \"employment history\", \"work experience\",\n",
    "        \"professional experience\", \"career summary\", \"professional background\",\n",
    "        \"job experience\", \"internships\", \"relevant experience\",\n",
    "        \"contract work\", \"military experience\", \"volunteer work\",\n",
    "        \"experience\"\n",
    "    ]\n",
    "    \n",
    "    # Regex pattern for experience section headers\n",
    "    experience_section_pattern = '|'.join(experience_section_keywords)\n",
    "\n",
    "    # Define date patterns for recognizing job durations\n",
    "    date_patterns = [\n",
    "        r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\s*[-‚Äì‚Äîto]+\\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s*\\d{4}\",\n",
    "        r\"\\b\\d{4}\\s*[-‚Äì‚Äîto]+\\s*\\d{4}\\b\",\n",
    "        r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\s*[-‚Äì‚Äîto]+\\s*Present\",\n",
    "        r\"\\b\\d{4}\\s*[-‚Äì‚Äîto]+\\s*Present\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Patterns for extracting company names\n",
    "    company_patterns = r\"(?:at|with|for)\\s+([A-Z][\\w&,. ]+)\"\n",
    "    \n",
    "    # Patterns for extracting location information\n",
    "    location_patterns = [\n",
    "        r\"\\bin\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b\",\n",
    "        r\"\\b([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\s+(USA|Inc|Ltd|LLC|Co)\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Patterns for extracting responsibilities\n",
    "    responsibility_patterns = [\n",
    "        r\"‚Ä¢\\s*.*\",  # Bullet point responsibilities\n",
    "        r\"(?<=\\n)-\\s*.*\",  # Responsibilities starting with a hyphen\n",
    "        r\"(Managed|Developed|Led|Coordinated|Implemented|Oversaw|Supervised)\\s+.*\"\n",
    "    ]\n",
    "\n",
    "    # Combine all location patterns into a single regex pattern\n",
    "    location_regex = '|'.join(location_patterns)\n",
    "\n",
    "    # Debugging: Print the extracted text\n",
    "    print(\"Extracted Text:\\n\", text)  # Print the full text for review\n",
    "\n",
    "    # Search for the start of the experience section\n",
    "    experience_start = re.search(experience_section_pattern, text, re.IGNORECASE)\n",
    "    if not experience_start:\n",
    "        print(\"Experience section not found.\")\n",
    "        return []  # Return empty list if no experience section is found\n",
    "\n",
    "    # Extract content starting from the identified section\n",
    "    content = text[experience_start.start():]\n",
    "\n",
    "    # Extract dates to isolate job entries\n",
    "    dates = re.findall('|'.join(date_patterns), content)\n",
    "    job_entries = re.split('|'.join(date_patterns), content)\n",
    "\n",
    "    # List to hold extracted job experiences\n",
    "    experiences = []\n",
    "\n",
    "    # Loop through each identified job entry and its corresponding date\n",
    "    for date, job in zip(dates, job_entries):\n",
    "        job_clean = job.strip()\n",
    "        print(f\"Extracting job entry: '{job_clean}'\")  # Debugging print\n",
    "        \n",
    "        # Skip if the job entry is a line with only hyphens, underscores, or dots\n",
    "        if re.match(r'^[\\-_]+$', job_clean) or re.match(r'^[.]+$', job_clean) or not job_clean:\n",
    "            print(\"Skipping entry due to irrelevant line.\")\n",
    "            continue  # Skip this entry\n",
    "\n",
    "        # Extract job title\n",
    "        job_title_match = re.search(r'([A-Z][a-zA-Z\\s]+(?:[ -][A-Z][a-zA-Z]*)*)', job_clean)\n",
    "        job_title = job_title_match.group(0) if job_title_match else 'Not Found'\n",
    "\n",
    "        # Extract company name\n",
    "        company_match = re.search(company_patterns, job_clean)\n",
    "        company = company_match.group(1) if company_match else 'Not Found'\n",
    "\n",
    "        # Extract location\n",
    "        location_match = re.search(location_regex, job_clean)\n",
    "        location = location_match.group(1) if location_match else 'Not Found'\n",
    "\n",
    "        # Extract responsibilities\n",
    "        responsibilities = re.findall('|'.join(responsibility_patterns), job_clean)\n",
    "        responsibilities_text = ' '.join(responsibilities).strip()\n",
    "\n",
    "        # Store the extracted data in the experiences list\n",
    "        experiences.append({\n",
    "            \"Job Title\": job_title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location,\n",
    "            \"Employment Dates\": date.strip(),\n",
    "            \"Responsibilities\": responsibilities_text\n",
    "        })\n",
    "\n",
    "    if not experiences:\n",
    "        print(\"No experiences found.\")\n",
    "        return [{\"Job Title\": \"Not Found\", \"Company\": \"Not Found\", \"Location\": \"Not Found\", \"Employment Dates\": \"Not Found\", \"Responsibilities\": \"Not Found\"}]\n",
    "\n",
    "    return experiences\n",
    "\n",
    "# Function to extract education details\n",
    "def extract_education(text):\n",
    "    education_section_keywords = [\n",
    "        \"education\", \"academic background\", \"educational qualifications\",\n",
    "        \"degrees\", \"certifications\"\n",
    "    ]\n",
    "\n",
    "    # Search for the start of the education section\n",
    "    education_start = re.search('|'.join(education_section_keywords), text, re.IGNORECASE)\n",
    "    if not education_start:\n",
    "        return None  # Return None if no education section is found\n",
    "\n",
    "    content = text[education_start.start():]\n",
    "\n",
    "    # This regex is simplified. You can enhance it to capture more specific patterns\n",
    "    education_patterns = r\"(\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b) +(\\d{4}|\\b[0-9]{4})?[-‚Äì‚Äîto]*(\\b[0-9]{4})?\"\n",
    "    \n",
    "    matches = re.findall(education_patterns, content)\n",
    "    education_details = []\n",
    "\n",
    "    for match in matches:\n",
    "        degree = match[0].strip()\n",
    "        start_year = match[1].strip() if match[1] else \"Not specified\"\n",
    "        end_year = match[2].strip() if match[2] else \"Not specified\"\n",
    "\n",
    "        education_details.append({\n",
    "            \"Degree\": degree,\n",
    "            \"Start Year\": start_year,\n",
    "            \"End Year\": end_year\n",
    "        })\n",
    "\n",
    "    return education_details\n",
    "\n",
    "# Function to extract languages\n",
    "def extract_languages(text):\n",
    "    language_section_keywords = [\n",
    "        \"languages\", \"language proficiency\", \"spoken languages\", \"language skills\"\n",
    "    ]\n",
    "\n",
    "    # Search for the start of the languages section\n",
    "    language_start = re.search('|'.join(language_section_keywords), text, re.IGNORECASE)\n",
    "    if not language_start:\n",
    "        return None  # Return None if no languages section is found\n",
    "\n",
    "    content = text[language_start.start():]\n",
    "    language_patterns = r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b\"  # This pattern can be improved based on the expected language format\n",
    "\n",
    "    matches = re.findall(language_patterns, content)\n",
    "    languages = [match.strip() for match in matches]\n",
    "\n",
    "    return languages if languages else [\"Not Found\"]\n",
    "\n",
    "# Function to extract certificates\n",
    "def extract_certificates(text):\n",
    "    certificate_section_keywords = [\n",
    "        \"certificates\", \"certification\", \"professional certifications\",\n",
    "        \"certification courses\"\n",
    "    ]\n",
    "\n",
    "    # Search for the start of the certificates section\n",
    "    certificate_start = re.search('|'.join(certificate_section_keywords), text, re.IGNORECASE)\n",
    "    if not certificate_start:\n",
    "        return None  # Return None if no certificates section is found\n",
    "\n",
    "    content = text[certificate_start.start():]\n",
    "    certificate_patterns = r\"\\b([A-Z][a-zA-Z\\s]+)\\b\"  # Simplified regex pattern to capture certificates\n",
    "\n",
    "    matches = re.findall(certificate_patterns, content)\n",
    "    certificates = [match.strip() for match in matches]\n",
    "\n",
    "    return certificates if certificates else [\"Not Found\"]\n",
    "\n",
    "# Main function to extract data from resumes\n",
    "def extract_data_from_resumes(resume_folder):\n",
    "    all_extracted_data = []\n",
    "    \n",
    "    # Extract all PDF files from the specified folder\n",
    "    resume_paths = [os.path.join(resume_folder, f) for f in os.listdir(resume_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_path in resume_paths:\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        name = extract_name(text)\n",
    "        email = extract_email(text)\n",
    "        phone = extract_phone_number(text)\n",
    "        address = extract_address(text)\n",
    "        profile_summary = extract_profile_summary(text)\n",
    "        experience = extract_experience(text)\n",
    "        education = extract_education(text)\n",
    "        languages = extract_languages(text)\n",
    "        certificates = extract_certificates(text)\n",
    "\n",
    "        extracted_data = {\n",
    "            \"Name\": name,\n",
    "            \"Email\": email,\n",
    "            \"Phone\": phone,\n",
    "            \"Address\": address,\n",
    "            \"Profile Summary\": profile_summary,\n",
    "            \"Experience\": experience,\n",
    "            \"Education\": education,\n",
    "            \"Languages\": languages,\n",
    "            \"Certificates\": certificates\n",
    "        }\n",
    "        all_extracted_data.append(extracted_data)\n",
    "\n",
    "    return all_extracted_data\n",
    "\n",
    "# Function to save extracted data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Append or create new file based on whether it exists\n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)  # Append without header\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)  # Create a new file with header\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the folder containing resumes and the output CSV file name\n",
    "    resume_folder = \"C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\New folder\\\\resume corpus\"  # Replace with the path to your resumes folder\n",
    "    output_csv_file = \"output7.csv\"  # Desired CSV output filename\n",
    "    \n",
    "    extracted_info = extract_data_from_resumes(resume_folder)\n",
    "    save_to_csv(extracted_info, output_csv_file)\n",
    "\n",
    "    print(f\"Data extracted and saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd881991-a5d5-439d-9219-1f271d46b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              File Name                                         Experience\n",
      "0  AnuvaGoyal_Latex.pdf  Summer Intern\\nGenisup India Pvt. Ltd., Hosur,...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "def extract_experience(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the experience section from a PDF resume.\n",
    "    \n",
    "    Parameters:\n",
    "    pdf_path (str): The path to the PDF resume file.\n",
    "    \n",
    "    Returns:\n",
    "    str: A string containing the extracted experience details.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    \n",
    "    # Extract text from the PDF\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + '\\n'\n",
    "\n",
    "    # Split text into lines for easier processing\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    experience_list = []\n",
    "    capture = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Normalize spaces\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Check if we have reached the Experience section\n",
    "        if 'EXPERIENCE' in line.upper():\n",
    "            capture = True\n",
    "            continue\n",
    "        \n",
    "        # If in the experience section, check for end condition\n",
    "        if capture:\n",
    "            # Check if line has only a straight line or dotted line\n",
    "            if re.match(r'^[-‚Ä¢‚Äì]+$', line):\n",
    "                continue  # Skip this line\n",
    "            \n",
    "            # If we reach a non-experience line (like Projects, Education), we stop capturing\n",
    "            if any(keyword in line.upper() for keyword in ['PROJECTS', 'EDUCATION', 'TRAININGS']):\n",
    "                break\n",
    "            \n",
    "            # Otherwise, add the line to experience list\n",
    "            experience_list.append(line)\n",
    "    \n",
    "    # Combine the experience details into a single string\n",
    "    experience_details = \"\\n\".join(experience_list)\n",
    "    \n",
    "    return experience_details.strip()\n",
    "\n",
    "def process_resumes(resumes_folder):\n",
    "    \"\"\"\n",
    "    Processes all PDF resumes in a specified folder to extract experience.\n",
    "    \n",
    "    Parameters:\n",
    "    resumes_folder (str): The path to the folder containing resume PDFs.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the extracted experiences from each resume.\n",
    "    \"\"\"\n",
    "    experience_data = []\n",
    "\n",
    "    # Loop through each PDF file in the resumes folder\n",
    "    for filename in os.listdir(resumes_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(resumes_folder, filename)\n",
    "            experience = extract_experience(pdf_path)\n",
    "            experience_data.append({'File Name': filename, 'Experience': experience})\n",
    "\n",
    "    # Create a DataFrame from the experience data\n",
    "    df_experience = pd.DataFrame(experience_data)\n",
    "    return df_experience\n",
    "\n",
    "# Example usage\n",
    "resumes_folder = 'C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\New folder\\\\resume corpus'\n",
    "experience_df = process_resumes(resumes_folder)\n",
    "\n",
    "# Display the extracted experiences\n",
    "print(experience_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e827cf1f-eb0d-4632-97fc-0676d2f6b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           File Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Experience\n",
      "AnuvaGoyal_Latex.pdf Summer Intern\\nGenisup India Pvt. Ltd., Hosur, Tamil Nadu\\n·ΩåJune 2021 ‚Äì Aug 2021 Remote\\n‚Ä¢Internship on the topic NLP: Topic Modeling to assign the\\ntheme or topic for any news article on internet using Machine\\nLearning techniques.\\n‚Ä¢Worked on proxy rotation and Web Scraping\\n‚Ä¢Performed LDA Topic Modeling on ‚ÄúThe Hindu‚Äù news articles\\nand obtained precision score of 0.906.\\nIntern Trainee\\nVUGS Technologies Pvt. Ltd., Agra, Uttar Pradesh\\n·ΩåMay 2021 ‚Äì June 2021 Remote\\n‚Ä¢Built an OCR using Pytesseract and NER Text ClassiÔ¨Åcation\\nModel to categorize detected text into Name, E-mail Address,\\nPhone number and Date using NLTK,SpaCy and BERT\\n‚Ä¢Created an OCR for Handwritten text [A-Z, 0-9] using CNN\\narchitecture\\n‚Ä¢Built a Face Recognition Model and Face Mask Detection\\nModel using OpenCV and Haar Cascade ClassiÔ¨Åer.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "def extract_experience(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the experience section from a PDF resume.\n",
    "    \n",
    "    Parameters:\n",
    "    pdf_path (str): The path to the PDF resume file.\n",
    "    \n",
    "    Returns:\n",
    "    str: A string containing the extracted experience details.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    \n",
    "    # Extract text from the PDF\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + '\\n'\n",
    "\n",
    "    # Split text into lines for easier processing\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    experience_list = []\n",
    "    capture = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Normalize spaces\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Check if we have reached the Experience section\n",
    "        if 'EXPERIENCE' in line.upper():\n",
    "            capture = True\n",
    "            continue\n",
    "        \n",
    "        # If in the experience section, check for end condition\n",
    "        if capture:\n",
    "            # Check if line has only a straight line or dotted line\n",
    "            if re.match(r'^[-‚Ä¢‚Äì]+$', line):\n",
    "                continue  # Skip this line\n",
    "            \n",
    "            # If we reach a non-experience line (like Projects, Education), we stop capturing\n",
    "            if any(keyword in line.upper() for keyword in ['PROJECTS', 'EDUCATION', 'TRAININGS']):\n",
    "                break\n",
    "            \n",
    "            # Otherwise, add the line to experience list\n",
    "            experience_list.append(line)\n",
    "    \n",
    "    # Combine the experience details into a single string with full lines\n",
    "    experience_details = \"\\n\".join(experience_list)\n",
    "    \n",
    "    return experience_details.strip() if experience_details else \"No experience found.\"\n",
    "\n",
    "def process_resumes(resumes_folder):\n",
    "    \"\"\"\n",
    "    Processes all PDF resumes in a specified folder to extract experience.\n",
    "    \n",
    "    Parameters:\n",
    "    resumes_folder (str): The path to the folder containing resume PDFs.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the extracted experiences from each resume.\n",
    "    \"\"\"\n",
    "    experience_data = []\n",
    "\n",
    "    # Loop through each PDF file in the resumes folder\n",
    "    for filename in os.listdir(resumes_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(resumes_folder, filename)\n",
    "            experience = extract_experience(pdf_path)\n",
    "            experience_data.append({'File Name': filename, 'Experience': experience})\n",
    "\n",
    "    # Create a DataFrame from the experience data\n",
    "    df_experience = pd.DataFrame(experience_data)\n",
    "    return df_experience\n",
    "\n",
    "# Example usage\n",
    "resumes_folder = 'C:\\\\Users\\\\santhoshs.s\\\\jupyter\\\\New folder\\\\resume corpus'  # Change this to your folder path\n",
    "experience_df = process_resumes(resumes_folder)\n",
    "\n",
    "# Display the extracted experiences\n",
    "print(experience_df.to_string(index=False))  # Display without index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce795d8-eefc-44cc-a3d3-8439ab04d3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
